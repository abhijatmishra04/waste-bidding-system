# -*- coding: utf-8 -*-

"""
Streamlit UI (v1.45.1) for: LangGraph agent (PDF -> PNG -> LLM via RAI Gateway -> KV JSON)

Features
- Upload PDF + paste/upload RAI config (.cfg)
- Choose model route & optional profile
- Set extraction params (max pages, DPI, allowed keys, etc.)
- Preview rendered page images
- Run extraction (image-first with TEXT fallback) and view JSON
- Download KV JSON
- Rich logs panel (with live capture)
- Clean, modern UI with gradient header

NOTE: You need the backend packages available in your environment:
  streamlit==1.45.1, PyMuPDF, pdfplumber, pydantic>=2, langchain-core, langgraph, and your 'ssrai' SDK.
"""

import os
import io
import re
import json
import time
import base64
import shutil
import tempfile
import logging
from collections import OrderedDict
from typing import List, Optional, Any, Literal, OrderedDict as TOrderedDict, TypedDict

import streamlit as st

# ---------- Optional deps used by the agent ----------
try:
    import fitz  # PyMuPDF
    _HAVE_FITZ = True
except Exception:
    _HAVE_FITZ = False

try:
    import pdfplumber
except Exception as e:
    pdfplumber = None

try:
    from pydantic import BaseModel, Field, field_validator
except Exception as e:
    BaseModel = object  # soft fallback so the file can import; app will error gracefully
    Field = lambda *a, **k: None
    def field_validator(*a, **k):
        def _decor(fn):
            return fn
        return _decor

try:
    from langchain_core.prompts import PromptTemplate
    from langchain_core.output_parsers import PydanticOutputParser
except Exception:
    PromptTemplate = None
    PydanticOutputParser = None

try:
    from langgraph.graph import StateGraph, START, END
except Exception:
    StateGraph = None
    START = END = None

try:
    from ssrai import SSRAIClient
except Exception:
    SSRAIClient = None


# ======================== Pydantic models ========================

class KV(BaseModel):
    key: str = Field(..., description="Field name (normalized)")
    value: str = Field(..., description="Extracted value as plain text")
    page: int = Field(..., ge=1, description="1-based page number")
    confidence: float = Field(..., ge=0, le=1, description="0..1 model confidence")

    @field_validator("key")
    @classmethod
    def _strip_key(cls, v: str) -> str:
        return v.strip()


class ExtractedDoc(BaseModel):
    items: List[KV] = Field(default_factory=list)


# ======================== Logging ========================

class _ListLogHandler(logging.Handler):
    """Collects log messages into a list for display in Streamlit."""
    def __init__(self, collector: list, level=logging.NOTSET):
        super().__init__(level=level)
        self.collector = collector
        self._fmt = logging.Formatter("%(asctime)s | %(levelname)-8s | %(name)s | %(message)s",
                                      datefmt="%Y-%m-%d %H:%M:%S")

    def emit(self, record):
        try:
            self.collector.append(self._fmt.format(record))
        except Exception:
            pass


def setup_logger(level: str, log_file: Optional[str], live_collector: Optional[list] = None) -> logging.Logger:
    lvl = getattr(logging, (level or "INFO").upper(), logging.INFO)
    logger = logging.getLogger("pdf2json")
    logger.setLevel(lvl)
    logger.handlers.clear()

    fmt = logging.Formatter(
        "%(asctime)s | %(levelname)-8s | %(name)s | %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )

    ch = logging.StreamHandler()
    ch.setLevel(lvl)
    ch.setFormatter(fmt)
    logger.addHandler(ch)

    if log_file:
        os.makedirs(os.path.dirname(log_file) or ".", exist_ok=True)
        fh = logging.FileHandler(log_file, encoding="utf-8")
        fh.setLevel(lvl)
        fh.setFormatter(fmt)
        logger.addHandler(fh)

    if live_collector is not None:
        lh = _ListLogHandler(live_collector, level=lvl)
        logger.addHandler(lh)

    if lvl > logging.DEBUG:
        logging.getLogger("urllib3.connectionpool").setLevel(logging.WARNING)
        logging.getLogger("ssrai").setLevel(logging.INFO)
        logging.getLogger("pdfplumber").setLevel(logging.WARNING)

    return logger


# ======================== Helpers (non-graph) ========================

def build_prompts(allowed_keys: Optional[List[str]]):
    if PydanticOutputParser is None or PromptTemplate is None:
        raise RuntimeError("langchain-core is not installed. Please install 'langchain-core'.")
    parser = PydanticOutputParser(pydantic_object=ExtractedDoc)
    allowed_block = (
        "Only output keys from this list:\n- " + "\n- ".join(allowed_keys) + "\n"
        if allowed_keys else
        "Output any clearly labeled field names as keys.\n"
    )
    prompt_t = r"""
You are a precise information-extraction engine.
Given attached INVOICE PAGE IMAGES (PNG) or plain TEXT, extract explicit **key-value** pairs.

Rules:
- {{ allowed_keys_block }}
- Keep values verbatim; normalize whitespace only.
- If a field repeats across pages, still return it (we will dedupe later).
- If nothing certain is present, return {"items": []}.
- Estimate confidence between 0 and 1.
- Return **ONLY** valid JSON following the schema.

{{format_instructions}}
""".strip()
    strict_suffix = "\n\nREMINDER: Output JSON ONLY. No markdown, no prose, no comments."

    prompt = PromptTemplate(
        template=prompt_t,
        template_format="jinja2",
        input_variables=["allowed_keys_block"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
    ).format(allowed_keys_block=allowed_block)

    prompt_strict = PromptTemplate(
        template=prompt_t + strict_suffix,
        template_format="jinja2",
        input_variables=["allowed_keys_block"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
    ).format(allowed_keys_block=allowed_block)

    return prompt, prompt_strict, parser


def sanitize_json_text(text: str) -> str:
    """Extract the JSON object from model output, tolerating accidental wrappers."""
    if not isinstance(text, str):
        text = str(text)
    try:
        json.loads(text)
        return text
    except Exception:
        pass
    m = re.search(r"```json\s*(\{.*?\})\s*```", text, flags=re.DOTALL | re.IGNORECASE)
    if m:
        return m.group(1)
    s, e = text.find("{"), text.rfind("}")
    if s != -1 and e != -1 and e > s:
        return text[s:e+1]
    return text  # may still fail upstream; caller decides


def validate_model_route(cli: SSRAIClient, route: str, log: logging.Logger) -> None:
    """Preflight: check that the requested route exists; warn if not image-capable."""
    def _iter_model_rows():
        try:
            rows = cli.model.search(query=route)
        except Exception:
            rows = cli.model.search()

        # pandas DataFrame
        try:
            import pandas as pd  # noqa
            if hasattr(rows, "to_dict") and hasattr(rows, "columns"):
                cols = [str(c) for c in list(rows.columns)]
                name_col = None
                for c in ("modelName", "name", "model", "route"):
                    if c in cols:
                        name_col = c
                        break
                if name_col is not None:
                    for _, r in rows.iterrows():
                        yield {
                            "modelName": str(r.get(name_col)),
                            "features": r.get("features", {}) if "features" in cols else {},
                        }
                    return
        except Exception:
            pass

        # dict mapping
        if isinstance(rows, dict):
            for k, v in rows.items():
                yield {"modelName": str(k), "features": v}
            return

        # list/tuple
        if isinstance(rows, (list, tuple)):
            for r in rows:
                if isinstance(r, dict):
                    name = r.get("modelName") or r.get("name") or r.get("route") or r.get("model")
                    yield {"modelName": str(name), "features": r.get("features", {})}
                elif isinstance(r, str):
                    yield {"modelName": r, "features": {}}
                elif isinstance(r, (list, tuple)):
                    name = r[0] if r else ""
                    feats = r[1] if len(r) > 1 else {}
                    yield {"modelName": str(name), "features": feats}
                else:
                    yield {"modelName": str(r), "features": {}}
            return

        # fallback
        yield {"modelName": str(rows), "features": {}}

    models = list(_iter_model_rows())
    names = [m.get("modelName") for m in models]
    log.info("Found %d models matching the search criteria.", len(models))
    match = [m for m in models if (m.get("modelName") or "").strip().lower() == route.strip().lower()]
    if not match:
        raise ValueError(
            f"Model '{route}' not found on the gateway.\n"
            f"Available routes (first 10): {names[:10]} ..."
        )

    feats = (match[0].get("features") or {})
    looks_vision = any(k in str(feats).lower() for k in ["4o", "vision", "image", "multimodal"])
    if not looks_vision:
        log.warning(
            "Model '%s' may not be image-enabled (features=%s). "
            "Image path may fail; TEXT fallback is %s.",
            route, feats, "ON"
        )
    log.info("Using model '%s' (features=%s)", route, feats)


def pdf_to_png(pdf_path: str, max_pages: int, dpi: int,
               dump_images: Optional[str], keep_images: bool,
               log: logging.Logger) -> tuple[List[str], Optional[str]]:
    if dump_images:
        out_dir, temp_dir = dump_images, None
        os.makedirs(out_dir, exist_ok=True)
    else:
        temp_dir = tempfile.mkdtemp(prefix="pdfpng_")
        out_dir = temp_dir

    paths: List[str] = []
    t0 = time.perf_counter()
    if _HAVE_FITZ:
        log.info("Rendering PDF with PyMuPDF @%ddpi (max_pages=%d)...", dpi, max_pages)
        doc = fitz.open(pdf_path)
        total = min(doc.page_count, max_pages)
        log.info("PDF has %d pages; rendering %d.", doc.page_count, total)
        for i in range(total):
            page = doc.load_page(i)
            mat = fitz.Matrix(dpi / 72.0, dpi / 72.0)
            pix = page.get_pixmap(matrix=mat, alpha=False)
            out_path = os.path.join(out_dir, f"page_{i+1:03d}.png")
            pix.save(out_path)
            paths.append(out_path)
            log.debug("Rendered page %d -> %s (%dx%d)", i+1, out_path, pix.width, pix.height)
        doc.close()
    else:
        if pdfplumber is None:
            raise RuntimeError("Neither PyMuPDF (fitz) nor pdfplumber is available to render PDF.")
        log.info("Rendering PDF with pdfplumber @%ddpi (max_pages=%d)...", dpi, max_pages)
        with pdfplumber.open(pdf_path) as pdf:
            total = min(len(pdf.pages), max_pages)
            log.info("PDF has %d pages; rendering %d.", len(pdf.pages), total)
            for i in range(total):
                page = pdf.pages[i]
                img = page.to_image(resolution=dpi).original  # PIL.Image
                out_path = os.path.join(out_dir, f"page_{i+1:03d}.png")
                img.save(out_path, format="PNG")
                paths.append(out_path)
                log.debug("Rendered page %d -> %s (%s)", i+1, out_path, str(img.size))

    dt = (time.perf_counter() - t0) * 1000
    total_mb = sum(os.path.getsize(p) for p in paths) / (1024 * 1024) if paths else 0.0
    log.info("Rendered %d PNG(s) in %.1f ms (%.1f MB).", len(paths), dt, total_mb)

    # decide cleanup dir
    temp_render_dir = None if keep_images or dump_images else temp_dir
    return paths, temp_render_dir


def _image_url_part_from_b64(b64: str, mime: str = "image/png") -> dict:
    return {"type": "image_url", "image_url": {"url": f"data:{mime};base64,{b64}"}}


def paths_to_payloads(paths: List[str], log: logging.Logger, mime: str = "image/png") -> List[dict]:
    parts = []
    for p in paths:
        with open(p, "rb") as f:
            b64 = base64.b64encode(f.read()).decode("ascii")
        parts.append(_image_url_part_from_b64(b64, mime=mime))
        log.debug("Prepared %s (%s b64 chars) for image_url payload", os.path.basename(p), len(b64))
    return parts


def call_with_images(cli: SSRAIClient, model: str, prompt: str, paths: List[str],
                     max_tokens: int, temperature: float, log: logging.Logger) -> str:
    content = [{"type": "text", "text": prompt}] + paths_to_payloads(paths, log=log, mime="image/png")
    messages = [
        {"role": "user", "content": content},  # your working order
        {"role": "system", "content": "You are a precise data-extraction assistant. Output JSON ONLY."},
    ]
    log.info("Calling model with IMAGE payload â %d page image(s).", len(paths))
    t0 = time.perf_counter()
    resp = cli.chat.create(
        model=model,
        messages=messages,
        max_tokens=max_tokens,
        temperature=temperature,
        n=1,
    )
    log.info("RAI chat.create (IMAGE) ok in %.1f ms", (time.perf_counter()-t0)*1000)
    msg = resp["choices"][0]["message"]
    out = msg.get("content", "")
    if isinstance(out, list) and out and isinstance(out[0], dict):
        out = out[0].get("text", "")
    return str(out)


def normalize_ws(s: str) -> str:
    s = s.replace("\r", "\n")
    lines = [ln.strip() for ln in s.split("\n")]
    compact, blank = [], False
    for ln in lines:
        if not ln:
            if not blank:
                compact.append("")
            blank = True
        else:
            compact.append(ln)
            blank = False
    return "\n".join(compact).strip()


def text_blocks_from_pdf(pdf_path: str, max_pages: int, page_chunk_chars: int) -> List[str]:
    if pdfplumber is None:
        raise RuntimeError("TEXT fallback requires 'pdfplumber'. Please install it.")
    blocks: List[str] = []
    with pdfplumber.open(pdf_path) as pdf:
        total = min(len(pdf.pages), max_pages)
        for i in range(total):
            page = pdf.pages[i]
            txt = page.extract_text() or ""
            try:
                tables = page.extract_tables() or []
            except Exception:
                tables = []
            for t in tables:
                for row in (t or []):
                    if not row:
                        continue
                    cells = [(c or "").strip() for c in row]
                    if any(cells):
                        txt += "\n" + " | ".join(cells)
            txt = normalize_ws(txt)
            if not txt:
                continue
            for j in range(0, len(txt), page_chunk_chars):
                chunk = txt[j:j+page_chunk_chars]
                blocks.append(f"PAGE {i+1} CHUNK {j//page_chunk_chars+1}:\n{chunk}")
    return blocks


def call_text_only(cli: SSRAIClient, model: str, prompt: str, text_blocks: List[str],
                   max_tokens: int, temperature: float, log: logging.Logger) -> str:
    log.info("Calling model with TEXT fallback: %d block(s)", len(text_blocks))
    content = [{"type": "text", "text": prompt}]
    for blk in text_blocks:
        content.append({"type": "text", "text": blk})
    messages = [
        {"role": "system", "content": "You are a precise extraction engine. Reply ONLY with JSON."},
        {"role": "user", "content": content},
    ]
    t0 = time.perf_counter()
    resp = cli.chat.create(
        model=model, messages=messages, max_tokens=max_tokens, temperature=temperature, n=1
    )
    log.info("RAI chat.create (TEXT) ok in %.1f ms", (time.perf_counter()-t0)*1000)
    msg = resp["choices"][0]["message"]
    out = msg.get("content", "")
    if isinstance(out, list) and out and isinstance(out[0], dict):
        out = out[0].get("text", "")
    return str(out)


def merge_items(items: List[KV]) -> List[KV]:
    by_key: dict[str, KV] = {}
    for it in items:
        norm = re.sub(r"[^a-z0-9]+", " ", it.key.lower()).strip()
        if norm not in by_key:
            by_key[norm] = it
        else:
            keep = by_key[norm]
            if len(it.value) > len(keep.value) or it.confidence > keep.confidence:
                by_key[norm] = it
    return [by_key[k] for k in OrderedDict.fromkeys(by_key.keys())]


# ======================== LangGraph State ========================

class AgentState(TypedDict, total=False):
    # inputs / config
    cfg: str
    model: str
    pdf: str
    out: Optional[str]
    allowed_keys: Optional[List[str]]
    max_pages: int
    dpi: int
    max_tokens: int
    temperature: float
    dump_images: Optional[str]
    keep_images: bool
    text_fallback: bool
    page_chunk_chars: int
    profile: Optional[str]
    log_level: str
    log_file: Optional[str]

    # runtime
    log: Any
    cli: Any
    prompt: str
    prompt_strict: str
    parser: Any

    png_paths: List[str]
    temp_render_dir: Optional[str]
    path_taken: Literal["image", "image_failed", "text"]
    need_strict: bool
    raw: str
    payload: str
    doc: ExtractedDoc
    merged: List[KV]
    kv_map: TOrderedDict[str, str]


# ======================== LangGraph Nodes ========================

def node_init(state: AgentState, live_logs: Optional[list] = None) -> AgentState:
    log = setup_logger(state.get("log_level", "INFO"), state.get("log_file"), live_collector=live_logs)
    if SSRAIClient is None:
        raise RuntimeError("The 'ssrai' SDK is not installed. Please install and configure it.")
    cli = SSRAIClient(config_file=state["cfg"], profile=state.get("profile")) if state.get("profile") \
        else SSRAIClient(config_file=state["cfg"])
    validate_model_route(cli, state["model"], log)
    prompt, prompt_strict, parser = build_prompts(state.get("allowed_keys"))
    return {"log": log, "cli": cli, "prompt": prompt, "prompt_strict": prompt_strict, "parser": parser}


def node_render(state: AgentState) -> AgentState:
    paths, temp_dir = pdf_to_png(
        pdf_path=state["pdf"],
        max_pages=state["max_pages"],
        dpi=state["dpi"],
        dump_images=state.get("dump_images"),
        keep_images=state.get("keep_images", False),
        log=state["log"],
    )
    if not paths:
        raise RuntimeError("No PNGs produced from PDF.")
    return {"png_paths": paths, "temp_render_dir": temp_dir}


def node_try_image(state: AgentState) -> AgentState:
    log = state["log"]
    try:
        raw = call_with_images(
            cli=state["cli"], model=state["model"], prompt=state["prompt"],
            paths=state["png_paths"], max_tokens=state["max_tokens"],
            temperature=state["temperature"], log=log
        )
        return {"raw": raw, "path_taken": "image"}
    except Exception as e:
        log.warning("Image path failed: %s", e)
        return {"path_taken": "image_failed"}


def cond_after_image(state: AgentState) -> Literal["image_ok", "image_failed_no_fallback", "image_failed_with_fallback"]:
    if state["path_taken"] == "image":
        return "image_ok"
    # image failed:
    if not state.get("text_fallback", True):
        return "image_failed_no_fallback"
    return "image_failed_with_fallback"


def node_extract_text(state: AgentState) -> AgentState:
    text_blocks = text_blocks_from_pdf(
        pdf_path=state["pdf"],
        max_pages=state["max_pages"],
        page_chunk_chars=state["page_chunk_chars"]
    )
    if not text_blocks:
        raise RuntimeError("TEXT fallback found no extractable text in the PDF.")
    raw = call_text_only(
        cli=state["cli"], model=state["model"], prompt=state["prompt"],
        text_blocks=text_blocks, max_tokens=state["max_tokens"],
        temperature=state["temperature"], log=state["log"]
    )
    return {"raw": raw, "path_taken": "text", "need_strict": False}


def node_sanitize_parse(state: AgentState) -> AgentState:
    log = state["log"]
    payload = sanitize_json_text(state["raw"])
    try:
        doc = state["parser"].parse(payload)
        return {"payload": payload, "doc": doc}
    except Exception as e:
        log.warning("Parse failed: %s", e)
        # If we came from TEXT and haven't retried strictly, signal strict path
        if state.get("path_taken") == "text" and not state.get("need_strict"):
            return {"need_strict": True}
        # Otherwise, escalate
        raise


def cond_after_parse(state: AgentState) -> Literal["ok", "strict"]:
    return "strict" if state.get("need_strict") else "ok"


def node_text_strict(state: AgentState) -> AgentState:
    # Do a strict re-ask on TEXT path
    text_blocks = text_blocks_from_pdf(
        pdf_path=state["pdf"],
        max_pages=state["max_pages"],
        page_chunk_chars=state["page_chunk_chars"]
    )
    if not text_blocks:
        raise RuntimeError("Strict TEXT re-ask: no extractable text.")
    raw = call_text_only(
        cli=state["cli"], model=state["model"], prompt=state["prompt_strict"],
        text_blocks=text_blocks, max_tokens=state["max_tokens"],
        temperature=state["temperature"], log=state["log"]
    )
    return {"raw": raw, "need_strict": False}


def node_merge(state: AgentState) -> AgentState:
    items = (state["doc"].items or [])
    state["log"].info("Model returned %d item(s).", len(items))
    merged = merge_items(items)
    state["log"].info("After merge: %d unique key(s).", len(merged))
    for it in merged:
        state["log"].debug("KV: %r = %r (p%d, conf=%.2f)", it.key, it.value, it.page, it.confidence)
    kv_map = OrderedDict((it.key, it.value) for it in merged)
    return {"merged": merged, "kv_map": kv_map}


def node_cleanup(state: AgentState) -> AgentState:
    temp_dir = state.get("temp_render_dir")
    if temp_dir and not state.get("keep_images"):
        try:
            shutil.rmtree(temp_dir, ignore_errors=True)
            state["log"].debug("Deleted temp dir %s", temp_dir)
        except Exception as e:
            state["log"].warning("Failed to delete temp render dir: %s", e)
    return {}


# ======================== Build Graph ========================

def build_app_graph() -> "StateGraph":
    if StateGraph is None:
        raise RuntimeError("langgraph is not installed. Please install 'langgraph'.")
    g = StateGraph(AgentState)
    g.add_node("init", node_init)
    g.add_node("render", node_render)
    g.add_node("try_image", node_try_image)
    g.add_node("extract_text", node_extract_text)
    g.add_node("sanitize_parse", node_sanitize_parse)
    g.add_node("text_strict", node_text_strict)
    g.add_node("merge", node_merge)
    g.add_node("cleanup", node_cleanup)

    g.add_edge(START, "init")
    g.add_edge("init", "render")
    g.add_edge("render", "try_image")

    g.add_conditional_edges(
        "try_image",
        cond_after_image,
        {
            "image_ok": "sanitize_parse",
            "image_failed_with_fallback": "extract_text",
            "image_failed_no_fallback": END,
        },
    )

    g.add_conditional_edges(
        "sanitize_parse",
        cond_after_parse,
        {
            "ok": "merge",
            "strict": "text_strict",
        },
    )
    g.add_edge("text_strict", "sanitize_parse")
    g.add_edge("merge", "cleanup")
    g.add_edge("cleanup", END)

    return g


# ======================== Streamlit App ========================

st.set_page_config(
    page_title="PDF â KV JSON (LangGraph + RAI)",
    page_icon="ð",
)

# ---- Custom CSS for a visually appealing UI ----
st.markdown(
    """
    <style>
    :root {
        --brand-grad: linear-gradient(90deg, #6a11cb 0%, #2575fc 100%);
    }
    .gradient-header {
        background: var(--brand-grad);
        border-radius: 14px;
        padding: 18px 22px;
        color: white;
        margin-bottom: 14px;
        box-shadow: 0 8px 24px rgba(0,0,0,.12);
    }
    .gradient-header h1, .gradient-header h3, .gradient-header p {
        color: white !important;
        margin: 0;
        padding: 0;
    }
    .small-note {
        opacity: .9;
        font-size: 0.88rem;
    }
    .stProgress > div > div > div > div {
        background: var(--brand-grad);
    }
    .metric-card {
        background: #0f172a0a;
        border: 1px solid rgba(99,102,241,.25);
        border-radius: 12px;
        padding: 14px;
    }
    .code-like {
        background: #0b1021;
        color: #e5e7eb;
        font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
        border-radius: 10px;
        padding: 12px;
        white-space: pre-wrap;
        border: 1px solid #1f2937;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

st.markdown(
    """
    <div class="gradient-header">
      <h1>ð PDF â ð KV JSON</h1>
      <p class="small-note">
        LangGraph agent â¢ RAI Gateway â¢ Image-first with TEXT fallback â¢ Confidence-aware merge
      </p>
    </div>
    """,
    unsafe_allow_html=True,
)

# ---- Sidebar Controls ----
with st.sidebar:
    st.header("âï¸ Configuration")
    st.caption("Paste/upload your RAI config and choose the model route.")
    cfg_file = st.file_uploader("RAI .cfg file", type=["cfg", "ini"], accept_multiple_files=False)
    cfg_text_default = "[DEFAULT]\n# paste your gateway config here\n# endpoint=...\n# api_key=...\n"
    cfg_text = st.text_area("...or paste config", value=cfg_text_default, height=140)

    model_route = st.text_input("Model route", value="azure-openai.gpt-4o", help="Must exist on your RAI gateway")
    profile = st.text_input("Optional RAI profile", value="", help="e.g., NO_GUARDRAIL")

    st.divider()
    st.subheader("Extraction Settings")
    max_pages = st.slider("Max pages", 1, 50, 10, help="Number of pages to consider")
    dpi = st.slider("Render DPI (PNG)", 72, 400, 200, step=4)
    max_tokens = st.slider("Max tokens (model output)", 256, 4096, 1300, step=64)
    temperature = st.slider("Temperature", 0.0, 1.0, 0.0, step=0.05)
    text_fallback = st.checkbox("Enable TEXT fallback", value=True)
    page_chars = st.number_input("Page chunk chars (TEXT)", value=12000, min_value=1000, step=500)

    allowed_keys_str = st.text_area("Allowed keys (one per line, optional)", value="", height=120)
    keep_images = st.checkbox("Keep temp images", value=False, help="If off, temp renders are deleted after run")

    st.divider()
    log_level = st.selectbox("Log level", ["DEBUG","INFO","WARNING","ERROR"], index=1)
    log_file = st.text_input("Optional log file path", value="")

# ---- Main Input: PDF ----
uploaded_pdf = st.file_uploader("ð Upload PDF", type=["pdf"], accept_multiple_files=False)

col_run1, col_run2 = st.columns([1,1])
with col_run1:
    run_btn = st.button("â¶ï¸ Extract to JSON", type="primary", use_container_width=True)
with col_run2:
    demo_note = st.empty()

# ---- Preview + Results containers ----
preview_area = st.container()
metrics_container = st.container()
json_container = st.container()
images_container = st.container()
logs_container = st.container()

# ---- Utility: Persist uploaded files to temp paths ----
def _persist_to_tmp(upload, suffix=""):
    if upload is None:
        return None
    data = upload.read()
    fd, path = tempfile.mkstemp(prefix="st_upload_", suffix=suffix)
    with os.fdopen(fd, "wb") as f:
        f.write(data)
    return path

# ---- Run Extraction ----
if run_btn:
    # Guard checks
    errors = []
    if uploaded_pdf is None:
        errors.append("Please upload a PDF to proceed.")
    if SSRAIClient is None:
        errors.append("The 'ssrai' SDK is not available. Install and configure it in your environment.")
    if StateGraph is None or PromptTemplate is None or PydanticOutputParser is None:
        errors.append("Please install dependencies: 'langgraph' and 'langchain-core' (and Pydantic v2).")
    if pdfplumber is None and not _HAVE_FITZ:
        errors.append("Install either PyMuPDF (fitz) or pdfplumber to render/extract pages.")

    if errors:
        for e in errors:
            st.error(e)
    else:
        # Prepare config file path
        cfg_path = None
        if cfg_file is not None:
            cfg_path = _persist_to_tmp(cfg_file, suffix=".cfg")
        else:
            # write pasted text to temp file
            fd, cfg_path = tempfile.mkstemp(prefix="rai_cfg_", suffix=".cfg")
            with os.fdopen(fd, "w", encoding="utf-8") as f:
                f.write(cfg_text or "")

        pdf_path = _persist_to_tmp(uploaded_pdf, suffix=".pdf")

        # Allowed keys (optional)
        allowed_keys = None
        if allowed_keys_str.strip():
            allowed_keys = [ln.strip() for ln in allowed_keys_str.splitlines() if ln.strip()]

        # Build initial state
        out_json_path = os.path.join(tempfile.gettempdir(), f"kv_{int(time.time())}.json")
        init_state = {
            "cfg": cfg_path,
            "model": model_route.strip(),
            "pdf": pdf_path,
            "out": out_json_path,
            "allowed_keys": allowed_keys,
            "max_pages": int(max_pages),
            "dpi": int(dpi),
            "max_tokens": int(max_tokens),
            "temperature": float(temperature),
            "dump_images": None,  # Streamlit preview shows rendered images directly
            "keep_images": bool(keep_images),
            "text_fallback": bool(text_fallback),
            "page_chunk_chars": int(page_chars),
            "profile": profile.strip() or None,
            "log_level": log_level,
            "log_file": log_file.strip() or None,
        }

        # Live logs buffer
        live_logs: list[str] = []

        # Render a progress UI
        with st.status("Working on itâ¦", expanded=True) as status:
            st.write("ð§ Initializing gateway & prompts")
            t0 = time.perf_counter()
            try:
                app_graph = build_app_graph().compile()
                # inject live logs into node_init via closure
                def _init_with_logs(state):
                    return node_init(state, live_logs=live_logs)
                app_graph.nodes["init"].fn = _init_with_logs  # type: ignore
            except Exception as e:
                st.error(f"Failed to build LangGraph: {e}")
                status.update(label="â Failed", state="error")
                st.stop()

            # Show a preview of first page render (fast, low DPI for preview only)
            if uploaded_pdf is not None:
                with preview_area:
                    st.subheader("ð Preview")
                    try:
                        # temp quick render for preview
                        prev_paths, prev_tmp = pdf_to_png(pdf_path, max_pages=1, dpi=120,
                                                          dump_images=None, keep_images=True,
                                                          log=setup_logger("ERROR", None))
                        if prev_paths:
                            st.image(prev_paths[0], caption="Page 1 (quick preview)", use_container_width=True)
                    except Exception:
                        pass

            st.write("ð¼ï¸ Rendering PDF pages to PNGâ¦")
            try:
                # Run the graph to completion
                final_state: AgentState = app_graph.invoke(init_state)
            except Exception as e:
                st.error(f"Pipeline failed: {e}")
                # Dump any collected logs
                with logs_container:
                    st.subheader("ð Logs")
                    st.code("\n".join(live_logs[-800:]) or "(no logs)", language="")
                status.update(label="â Failed", state="error")
                st.stop()

            # Prepare results
            kv_map = final_state.get("kv_map")
            png_paths = final_state.get("png_paths", []) or []
            elapsed_ms = (time.perf_counter() - t0) * 1000.0

            if not kv_map:
                st.warning("No KV output produced (image path may have failed and TEXT fallback disabled).")
                status.update(label="â ï¸ Done with warnings", state="warning")
            else:
                status.update(label="â Done", state="complete")

        # --- Metrics ---
        with metrics_container:
            st.subheader("ð Run Metrics")
            c1, c2, c3 = st.columns(3)
            with c1:
                st.metric("Unique keys", value=str(len(kv_map)))
            with c2:
                st.metric("Pages processed", value=str(min(max_pages, len(png_paths)) if png_paths else max_pages))
            with c3:
                st.metric("Elapsed (ms)", value=f"{elapsed_ms:.0f}")

        # --- JSON Result ---
        with json_container:
            st.subheader("ð§¾ Extracted KV JSON")
            st.json(kv_map or {}, expanded=False)
            if kv_map:
                b = io.BytesIO(json.dumps(kv_map, indent=2, ensure_ascii=False).encode("utf-8"))
                st.download_button("ð¾ Download JSON", data=b, file_name="extracted.kv.json", mime="application/json")

        # --- Rendered Page Images ---
        if png_paths:
            with images_container:
                st.subheader("ð¼ï¸ Rendered Pages")
                for p in png_paths[:12]:  # show up to 12 for sanity
                    st.image(p, caption=os.path.basename(p), use_container_width=True)

        # --- Logs ---
        with logs_container:
            st.subheader("ð Logs")
            st.code("\n".join(live_logs[-2000:]) or "(no logs)", language="")

# ---- Footer hint ----
st.caption("Tip: Tune 'Allowed keys' to control extraction. Leave empty to allow any detected fields.")

