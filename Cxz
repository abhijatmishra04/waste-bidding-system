
package com.bofa.sst.batch.core;

import com.bofa.sst.batch.common.CustaggBatchConstants;
import com.bofa.sst.batch.common.CustaggBatchException;
import com.bofa.sst.batch.common.FieldOffsetTable;
import com.bofa.sst.batch.dto.CustaggRecordsDTO;
import com.bofa.sst.batch.util.CompCharaterUtil; // adjust to your util’s actual package
import lombok.extern.log4j.Log4j2;
import org.springframework.batch.item.ExecutionContext;
import org.springframework.batch.item.ItemReader;
import org.springframework.batch.item.ItemStream;
import org.springframework.batch.item.ItemStreamException;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;
import java.nio.file.Paths;
import java.nio.file.StandardOpenOption;

/**
 * High-performance reader for fixed-length “comp” files.
 *
 * • Uses NIO FileChannel + single ByteBuffer per record  
 * • Seeks to (fromLine–1) * RECORD_SIZE at open, reads exactly RECORD_SIZE bytes each read()  
 * • Parses:
 *     – productCode (bytes 0–3)  
 *     – numericEntity (packed BCD via CompCharaterUtil)  
 *     – accountNumber (offset/length lookup via FieldOffsetTable)  
 * • Throws CustaggBatchException on any I/O or partial-record error
 */
@Log4j2
public class CustaggCompFileReader
        implements ItemReader<CustaggRecordsDTO>, ItemStream {

    private FileChannel channel;
    private long cursor, limit;
    private final ByteBuffer buffer =
            ByteBuffer.allocate(CustaggBatchConstants.RECORD_SIZE);

    @Override
    public void open(ExecutionContext ctx) throws ItemStreamException {
        try {
            cursor = ctx.getLong("fromLine");
            limit  = ctx.getLong("toLine");
            String filePath = ctx.getString("dataFile");

            channel = FileChannel.open(
                Paths.get(filePath),
                StandardOpenOption.READ
            );
            channel.position((cursor - 1) * CustaggBatchConstants.RECORD_SIZE);

            log.info("Reader opened on file={}, processing records {}–{}",
                     filePath, cursor, limit);
        } catch (IOException e) {
            throw new CustaggBatchException("Failed to open comp file reader", e);
        }
    }

    @Override
    public CustaggRecordsDTO read() throws Exception {
        if (cursor > limit) {
            return null;  // this partition is done
        }

        buffer.clear();
        int bytesRead;
        try {
            bytesRead = channel.read(buffer);
        } catch (IOException e) {
            throw new CustaggBatchException("I/O error at record index " + cursor, e);
        }

        if (bytesRead == -1) {
            return null;  // unexpected EOF
        }
        if (bytesRead != CustaggBatchConstants.RECORD_SIZE) {
            throw new CustaggBatchException("Incomplete record @ index " + cursor);
        }

        buffer.flip();
        byte[] record = new byte[CustaggBatchConstants.RECORD_SIZE];
        buffer.get(record);

        // parse fields
        String product = new String(record, 0, 3, CustaggBatchConstants.CHARSET).trim();
        String entity  = CompCharaterUtil.packedToString(record, 2, 2).trim();
        String accNo   = FieldOffsetTable.extractAccountNumber(product, record);

        if (log.isTraceEnabled()) {
            log.trace("Record {} → prod='{}', ent='{}', acc='{}'",
                      cursor, product, entity, accNo);
        }

        cursor++;
        return new CustaggRecordsDTO(product, entity, accNo);
    }

    @Override
    public void update(ExecutionContext executionContext) throws ItemStreamException {
        // no state to store between checkpoints
    }

    @Override
    public void close() throws ItemStreamException {
        if (channel != null) {
            try {
                channel.close();
            } catch (IOException e) {
                log.warn("Error closing FileChannel", e);
            }
        }
    }
}
