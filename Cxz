#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAI Gateway smoke test using PAT-style config keys:
host, auth_type=pat, rai_client_id, eam_consumer_key, eam_consumer_secret,
api_version, cloud_provider=azure

Usage:
  python rai_smoketest_pat.py
"""

import sys, tempfile, textwrap

# --- FILL THESE WITH REAL VALUES (TEST-ONLY!) -------------------------------
HOST                 = "https://<your-rai-gateway-host>"   # e.g., https://rai.company.com
AUTH_TYPE            = "pat"                                # per your screenshot
RAI_CLIENT_ID        = "<your_rai_client_id>"
EAM_CONSUMER_KEY     = "<your_apigee_or_eam_consumer_key>"
EAM_CONSUMER_SECRET  = "<your_apigee_or_eam_consumer_secret>"
API_VERSION          = "1.0"
CLOUD_PROVIDER       = "azure"

# Model route name must match a key in [routes] below (configured in your gateway)
MODEL_ROUTE          = "azure-openai.gpt4-32k"
AZURE_DEPLOYMENT     = "<your_azure_deployment_name>"       # the deployment name registered in RAI
AZURE_API_VERSION    = "2024-06-01"                         # provider API version used for this route
# ----------------------------------------------------------------------------


def _write_cfg() -> str:
    """
    Create a temp Option-A .cfg using *your* PAT-style keys from the screenshot.
    """
    cfg_text = textwrap.dedent(f"""\
    [gateway]
    host = {HOST}
    auth_type = {AUTH_TYPE}
    rai_client_id = {RAI_CLIENT_ID}
    eam_consumer_key = {EAM_CONSUMER_KEY}
    eam_consumer_secret = {EAM_CONSUMER_SECRET}
    api_version = {API_VERSION}
    cloud_provider = {CLOUD_PROVIDER}

    [routes]
    # Left side is the 'model_name' you'll pass to ssc.llm(...)
    {MODEL_ROUTE} = provider=azure-openai; deployment={AZURE_DEPLOYMENT}; api_version={AZURE_API_VERSION}

    [profiles]
    DEFAULT = content_filter=default; pii_mask=off
    WITH_GUARDRAIL = content_filter=on; pii_mask=on; log_prompt=redact
    """)
    tf = tempfile.NamedTemporaryFile(delete=False, suffix=".cfg")
    tf.write(cfg_text.encode("utf-8"))
    tf.flush()
    tf.close()
    return tf.name


def main():
    try:
        cfg_path = _write_cfg()

        # Lazy imports for clear error messages if deps are missing
        from ssrai import SSRAIClient
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser

        # Build client from temp config
        ssc = SSRAIClient(config_file=cfg_path)  # profile=None for raw test

        # Get a LangChain LLM from RAI and run a tiny prompt
        llm = ssc.llm(
            "langchain",
            MODEL_ROUTE,
            temperature=0.0,
            n=1,
            max_tokens=64,
        )

        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a health check. Reply exactly: 'pong {route}'."),
            ("user",   "route={{route}}")
        ])
        chain = prompt | llm | StrOutputParser()

        print("=== RAI PAT Smoke Test ===")
        out = chain.invoke({"route": MODEL_ROUTE})
        print(out.strip())

        # Minimal success check
        if "pong" in out.lower():
            print("✅ RAI model call succeeded.")
            sys.exit(0)
        else:
            print("⚠️ Response received, but didn't match expected 'pong'. Check guardrails/profile/route.")
            sys.exit(0)

    except Exception as e:
        print(f"❌ Smoke test failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
