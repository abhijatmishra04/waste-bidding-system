import os
import zipfile
import tempfile
import javalang
import yaml
import logging
import json
import networkx as nx
from pathlib import Path
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import JSONResponse
import io

# New imports for parsing properties and YAML files
from configparser import ConfigParser
import re
import traceback

# Load environment variables
load_dotenv()

# Set up the logger with enhanced formatting and handlers
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
# Create handlers
file_handler = logging.FileHandler('groq_api_log.log')
stream_handler = logging.StreamHandler()
# Create formatters and add to handlers
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)
stream_handler.setFormatter(formatter)
# Add handlers to the logger
if not logger.handlers:
    logger.addHandler(file_handler)
    logger.addHandler(stream_handler)

# Configure security settings
ALLOWED_FILE_TYPES = ['zip']
MAX_UPLOAD_SIZE = 50 * 1024 * 1024  # 50 MB

# Use environment variable for API key
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
if not GROQ_API_KEY:
    logger.error("Groq API key not found. Please set the GROQ_API_KEY environment variable.")
    raise ValueError("GROQ_API_KEY is not set.")

# Initialize FastAPI app
app = FastAPI()

# Configuration and constants
MAX_TOKENS = 5000000  # Token limit for prompt generation
DEFAULT_CONFIG = {
    'annotation_filters': [],
    'dependency_filters': {'include': [], 'exclude': []},
    'file_path_filters': {'include': [], 'exclude': []},
    'complexity_threshold': 10,
    'method_length_threshold': 50,
    'class_size_threshold': 10,
    'batch_size': 50,
    'output_file': 'scan_summary.json'
}
config = DEFAULT_CONFIG.copy()

def count_tokens(text):
    """Estimate the number of tokens in the text."""
    return len(text) / 4

def load_configuration(config_content):
    """Load configuration from uploaded YAML content."""
    global config
    try:
        user_config = yaml.safe_load(config_content)
        config = {**DEFAULT_CONFIG, **user_config}
        logger.info("Successfully loaded configuration.")
    except yaml.YAMLError as e:
        logger.error(f"Error parsing configuration file: {e}")
        config = DEFAULT_CONFIG.copy()
    except Exception as e:
        logger.error(f"Error loading configuration: {e}")
        config = DEFAULT_CONFIG.copy()

def scan_project_async(project_dir: str, output_format: str = 'json', output_directory: str = None) -> Dict[str, Any]:
    """Scan the Java project asynchronously with filtering options."""
    scan_summary = {}
    metadata_cache = {}
    try:
        logger.info(f"Starting to scan project at {project_dir}")

        java_files = list(Path(project_dir).rglob("*.java"))

        if not java_files:
            logger.error("No Java files found in the directory.")
            return {"error": "No Java files found in the project."}

        logger.info(f"Found {len(java_files)} Java files to scan.")

        # Apply file path filters
        java_files = apply_file_path_filters(java_files)

        batch_size = config.get('batch_size', 50)
        total_files = len(java_files)
        processed_files = 0

        # Parse configuration files
        configurations = parse_configuration_files(project_dir)

        # Parse dependency files
        dependencies = parse_dependency_files(project_dir)

        # Use ThreadPoolExecutor for concurrent scanning
        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
            futures = []
            for batch in [java_files[i:i + batch_size] for i in range(0, len(java_files), batch_size)]:
                futures.append(executor.submit(scan_batch, batch, scan_summary, metadata_cache, configurations, dependencies))

            # Update progress as futures complete
            for future in as_completed(futures):
                try:
                    future.result()
                    processed_files += batch_size
                    progress = processed_files / total_files
                    logger.info(f"Progress: {min(progress, 1.0)*100}%")
                except Exception as exc:
                    logger.error(f"Batch generated an exception: {exc}")

        # Detect cyclic dependencies after scanning all files
        detect_cyclic_dependencies(scan_summary)
        generate_report(output_format, project_dir, scan_summary, output_directory)
    except Exception as e:
        logger.error(f"Error during project scan: {e}")
        return {"error": f"Error during project scan: {e}"}

    return scan_summary

def parse_configuration_files(project_dir: str) -> Dict[str, Any]:
    """Parse application.properties and application.yml files."""
    configurations = {}
    # Parse application.properties files
    properties_files = list(Path(project_dir).rglob("application*.properties"))
    for prop_file in properties_files:
        config_parser = ConfigParser()
        with open(prop_file, 'r') as f:
            # Read the properties file as a .ini file after adding a section header
            content = '[DEFAULT]\n' + f.read()
            config_parser.read_string(content)
            configurations.update(dict(config_parser['DEFAULT']))

    # Parse application.yml files
    yml_files = list(Path(project_dir).rglob("application*.yml")) + list(Path(project_dir).rglob("application*.yaml"))
    for yml_file in yml_files:
        with open(yml_file, 'r') as f:
            yml_content = yaml.safe_load(f)
            configurations.update(yml_content)

    logger.info("Configuration files parsed.")
    return configurations

def parse_dependency_files(project_dir: str) -> Dict[str, Any]:
    """Parse pom.xml or build.gradle files to extract dependencies."""
    dependencies = {}
    # Parse pom.xml files
    pom_files = list(Path(project_dir).rglob("pom.xml"))
    for pom_file in pom_files:

        with open(pom_file, 'r') as f:
            content = f.read()
            dependency_matches = re.findall(r'<dependency>.*?</dependency>', content, re.DOTALL)
            for dep in dependency_matches:
                group_id = re.search(r'<groupId>(.*?)</groupId>', dep)
                artifact_id = re.search(r'<artifactId>(.*?)</artifactId>', dep)
                version = re.search(r'<version>(.*?)</version>', dep)
                if group_id and artifact_id and version:
                    key = f"{group_id.group(1)}:{artifact_id.group(1)}"
                    dependencies[key] = version.group(1)
    logger.info("Dependency files parsed.")
    return dependencies

def apply_file_path_filters(java_files: List[Path]) -> List[Path]:
    """Apply include/exclude filters to the list of Java files based on file paths."""
    include_filters = config['file_path_filters'].get('include', [])
    exclude_filters = config['file_path_filters'].get('exclude', [])

    filtered_files = []
    for java_file in java_files:
        file_str = str(java_file)
        if include_filters and not any(pattern in file_str for pattern in include_filters):
            continue
        if any(pattern in file_str for pattern in exclude_filters):
            continue
        filtered_files.append(java_file)

    logger.info(f"After applying filters, {len(filtered_files)} Java files will be scanned.")
    return filtered_files

def scan_batch(file_batch: List[Path], scan_summary, metadata_cache, configurations, dependencies):
    """Scan a batch of Java files."""
    for java_file in file_batch:
        scan_file(java_file, scan_summary, metadata_cache, configurations, dependencies)

def scan_file(java_file_path: Path, scan_summary, metadata_cache, configurations, dependencies):
    """Scan a single Java file for services, annotations, and dependencies."""
    try:
        with open(java_file_path, 'r', encoding='utf-8') as file:
            java_code = file.read()

        tree = javalang.parse.parse(java_code)

        imports = extract_imports(tree)
        logger.debug(f"Imports found in {java_file_path}: {imports}")

        for node in tree.types:
            if isinstance(node, javalang.tree.ClassDeclaration):
                scan_class(node, java_file_path, scan_summary, metadata_cache, configurations, dependencies, imports)

    except javalang.parser.JavaSyntaxError as e:
        logger.error(f"Syntax error in {java_file_path}: {e}")
    except Exception as e:
        logger.error(f"Error parsing {java_file_path}: {e}")

def scan_class(java_class, java_file_path: Path, scan_summary, metadata_cache, configurations, dependencies, imports):
    """Scan a Java class for advanced analysis: annotations, complexity, dependencies, and relationships."""
    try:
        class_name = java_class.name
        last_modified = os.path.getmtime(java_file_path)

        if class_name in metadata_cache and metadata_cache[class_name]['last_modified'] >= last_modified:
            logger.info(f"Skipping cached class {class_name}")
            return

        class_annotations = extract_annotations(java_class)
        if not apply_annotation_filters(class_annotations):
            logger.info(f"Class {class_name} filtered out by annotation filter")
            return

        # Determine if the class is a test class
        is_test_class = 'Test' in class_annotations or java_file_path.parts[-2].lower() == 'test'

        # Extract bounded context, API endpoints, and domain information
        bounded_context = detect_bounded_context(java_file_path)
        api_endpoints = extract_api_endpoints(java_class)
        domain = extract_domain_from_path(java_file_path)

        # Advanced analysis: Calculate cyclomatic complexity and detect code smells
        methods_complexity = calculate_methods_complexity(java_class)
        code_smells = detect_code_smells(java_class, methods_complexity, is_test_class)

        # Collect metadata for the class
        metadata = {
            'annotations': class_annotations,
            'component_type': detect_spring_component_type(java_class),
            'bounded_context': bounded_context,
            'api_endpoints': api_endpoints,
            'domain': domain,
            'methods_complexity': methods_complexity,
            'code_smells': code_smells,
            'layer': detect_layer(java_file_path),
            'file_path': str(java_file_path),
            'imports': imports  # Include imports in metadata
        }

        # Extract methods and fields
        methods = extract_methods(java_class)
        if methods:
            metadata['methods'] = methods

        fields = extract_fields(java_class)
        if fields:
            metadata['fields'] = fields

        # Detect service injections and relationships
        detect_service_injections(java_class, metadata)
        relationships = detect_relationships(java_class, metadata)
        if relationships:
            metadata['relationships'] = relationships

        # Extract Spring Beans
        if metadata['component_type'] in ["Configuration", "Component"]:
            beans = extract_spring_beans(java_class)
            if beans:
                metadata['spring_beans'] = beans

        # Security Analysis
        security_info = analyze_security(java_class)
        if security_info:
            metadata['security'] = security_info

        # AOP Analysis
        aspects = detect_aspects(java_class)
        if aspects:
            metadata['aspects'] = aspects

        # Profile and Conditional Beans
        profiles = extract_profiles(java_class)
        if profiles:
            metadata['profiles'] = profiles

        # Microservice Communication Analysis
        inter_service_calls = detect_inter_service_calls(java_class)
        if inter_service_calls:
            metadata['microservice_calls'] = inter_service_calls

        # Database Interaction Analysis
        db_interactions = analyze_database_interactions(java_class)
        if db_interactions:
            metadata['database_interactions'] = db_interactions

        # Database Entity Extraction
        database_entities = extract_database_entities(java_class)
        if database_entities:
            metadata['database_entities'] = database_entities

        # Messaging Patterns Detection
        messaging_usage = detect_messaging_usage(java_class)
        if messaging_usage:
            metadata['messaging_usage'] = messaging_usage

        # External API Calls Detection
        external_api_calls = detect_external_api_calls(java_class)
        if external_api_calls:
            metadata['external_api_calls'] = external_api_calls

        # Configuration Properties Extraction
        config_properties = extract_config_properties(java_class)
        if config_properties:
            metadata['config_properties'] = config_properties

        # Module Coupling Analysis
        module_coupling = analyze_module_coupling(java_class)
        if module_coupling:
            metadata['module_coupling'] = module_coupling

        # Documentation Analysis
        documentation_coverage = analyze_documentation(java_class)
        if documentation_coverage:
            metadata['documentation_coverage'] = documentation_coverage

        # Dependency Version Checks
        outdated_dependencies = check_dependency_versions(dependencies)
        if outdated_dependencies:
            metadata['outdated_dependencies'] = outdated_dependencies

        # Cache and summarize the metadata
        metadata_cache[class_name] = metadata
        scan_summary[class_name] = metadata

        logger.debug(f"Class {class_name} scanned with metadata: {metadata}")
    except Exception as e:
        logger.error(f"Error scanning class {java_class.name}: {e}")

def extract_imports(tree):
    """Extract imports from the CompilationUnit."""
    imports = []
    if hasattr(tree, 'imports'):
        for imp in tree.imports:
            imports.append(imp.path)
    return imports

def detect_spring_component_type(java_class):
    """Detect the Spring component type of the Java class based on its annotations."""
    component_types = [
        "SpringBootApplication", "Configuration", "RestController", "Controller",
        "Service", "Repository", "Component"
    ]

    for annotation in java_class.annotations:
        annotation_name = get_annotation_name(annotation)
        if annotation_name in component_types:
            return annotation_name

    return "Unknown"

def detect_service_injections(java_class, metadata):
    """Detect field-level, constructor-based, and method-based service injections."""
    inter_service_calls = []

    # Field injection
    if java_class.fields:
        for field in java_class.fields:
            if field.annotations:
                for annotation in field.annotations:
                    annotation_name = get_annotation_name(annotation)
                    if annotation_name in ["Autowired", "Inject", "Resource"]:
                        injected_service = field.type.name if field.type and field.type.name else 'Unknown'
                        inter_service_calls.append(injected_service)
                        logger.debug(f"Injected service found in {java_class.name}: {injected_service}")

    # Constructor injection
    if java_class.constructors:
        for constructor in java_class.constructors:
            if constructor.annotations:
                for annotation in constructor.annotations:
                    annotation_name = get_annotation_name(annotation)
                    if annotation_name in ["Autowired", "Inject"]:
                        for param in constructor.parameters:
                            injected_service = param.type.name if param.type and param.type.name else 'Unknown'
                            inter_service_calls.append(injected_service)
                            logger.debug(f"Injected service found in constructor of {java_class.name}: {injected_service}")

    # Method injection
    if java_class.methods:
        for method in java_class.methods:
            if method.annotations:
                for annotation in method.annotations:
                    annotation_name = get_annotation_name(annotation)
                    if annotation_name in ["Autowired", "Inject"]:
                        for param in method.parameters:
                            injected_service = param.type.name if param.type and param.type.name else 'Unknown'
                            inter_service_calls.append(injected_service)
                            logger.debug(f"Injected service found in method {method.name} of {java_class.name}: {injected_service}")

    if inter_service_calls:
        metadata['inter_service_calls'] = inter_service_calls

def extract_annotations(java_class):
    """Extract annotations from a Java class."""
    return [get_annotation_name(annotation) for annotation in java_class.annotations]

def get_annotation_name(annotation):
    """Get the name of an annotation."""
    if isinstance(annotation.name, str):
        return annotation.name
    elif hasattr(annotation.name, 'qualifier') and annotation.name.qualifier:
        return f"{annotation.name.qualifier}.{annotation.name.member}"
    else:
        return annotation.name.member

def extract_fields(java_class):
    """Extract fields and their metadata from a Java class."""
    fields = []
    if java_class.fields:
        for field in java_class.fields:
            field_info = {
                'names': [declarator.name for declarator in field.declarators],
                'type': field.type.name if field.type and isinstance(field.type.name, str) else 'Unknown',
                'annotations': [get_annotation_name(annotation) for annotation in field.annotations],
                'modifiers': list(field.modifiers)
            }
            fields.append(field_info)
    return fields

def extract_methods(java_class):
    """Extract methods from a Java class."""
    methods = []
    if java_class.methods:
        for method in java_class.methods:
            try:
                method_info = {
                    'name': method.name,
                    'return_type': method.return_type.name if method.return_type and isinstance(method.return_type.name, str) else 'void',
                    'parameters': [{'name': param.name,
                                    'type': param.type.name if param.type and param.type.name else 'Unknown'}
                                   for param in method.parameters],
                    'annotations': [get_annotation_name(annotation) for annotation in method.annotations],
                    'modifiers': list(method.modifiers),
                    'body_length': len(method.body) if method.body else 0
                }
                methods.append(method_info)
            except Exception as e:
                logger.error(f"Error extracting method {method.name}: {e}")
    return methods

def extract_spring_beans(java_class):
    """Extract beans defined in a @Configuration class."""
    beans = []
    if java_class.methods:
        for method in java_class.methods:
            for annotation in method.annotations:
                annotation_name = get_annotation_name(annotation)
                if annotation_name == "Bean":
                    bean_info = {
                        'name': method.name,
                        'return_type': method.return_type.name if method.return_type and isinstance(method.return_type.name, str) else 'Unknown',
                    }
                    beans.append(bean_info)
                    logger.debug(f"Bean found: {bean_info}")
    return beans

def extract_profiles(java_class):
    """Extract profiles and conditions under which beans are loaded."""
    profiles = []
    for annotation in java_class.annotations:
        annotation_name = get_annotation_name(annotation)
        if annotation_name in ["Profile", "ConditionalOnProperty", "Conditional"]:
            values = extract_annotation_value(annotation)
            profiles.extend(values)
    return profiles

def analyze_security(java_class):
    """Analyze security configurations in the class."""
    secured_methods = []
    if java_class.methods:
        for method in java_class.methods:
            for annotation in method.annotations:
                annotation_name = get_annotation_name(annotation)
                if annotation_name in ["PreAuthorize", "PostAuthorize", "Secured", "RolesAllowed"]:
                    secured_methods.append({
                        'method': method.name,
                        'annotation': annotation_name,
                        'value': extract_annotation_value(annotation)
                    })
    if secured_methods:
        return {'secured_methods': secured_methods}
    else:
        return None

def detect_aspects(java_class):
    """Detect aspects, pointcuts, and advice in the code."""
    aspects = []
    class_annotations = extract_annotations(java_class)
    if "Aspect" in class_annotations:
        aspect_info = {
            'name': java_class.name,
            'advices': []
        }
        if java_class.methods:
            for method in java_class.methods:
                for annotation in method.annotations:
                    annotation_name = get_annotation_name(annotation)
                    if annotation_name in ["Before", "After", "Around", "AfterReturning", "AfterThrowing", "Pointcut"]:
                        advice = {
                            'type': annotation_name,
                            'method': method.name,
                            'pointcut': extract_annotation_value(annotation)
                        }
                        aspect_info['advices'].append(advice)
            aspects.append(aspect_info)
            logger.debug(f"Aspect found: {aspect_info}")
    return aspects

def detect_inter_service_calls(java_class):
    """Detect inter-service communication patterns."""
    inter_service_calls = []
    if java_class.fields:
        for field in java_class.fields:
            field_type = field.type.name if field.type and field.type.name else ''
            if 'RestTemplate' in field_type or 'WebClient' in field_type:
                inter_service_calls.append({
                    'type': field_type,
                    'usage': 'HTTP Client'
                })
    if java_class.methods:
        for method in java_class.methods:
            if method.body:
                for path, node in method:
                    if isinstance(node, javalang.tree.MethodInvocation):
                        if node.member in ['getForObject', 'postForObject', 'exchange']:
                            inter_service_calls.append({
                                'method': method.name,
                                'call': node.member,
                                'target': node.arguments[0].value if node.arguments else 'Unknown',
                                'usage': 'HTTP Call'
                            })
    return inter_service_calls

def analyze_database_interactions(java_class):
    """Analyze database interactions, especially in repository interfaces."""
    db_interactions = []
    implements_interfaces = [impl.name for impl in java_class.implements] if java_class.implements else []
    if 'Repository' in extract_annotations(java_class) or 'JpaRepository' in implements_interfaces:
        if java_class.methods:
            for method in java_class.methods:
                method_info = {
                    'name': method.name,
                    'query': extract_query_from_method(method)
                }
                db_interactions.append(method_info)
    return db_interactions

def extract_query_from_method(method):
    """Extract query from method annotations or derive from method name."""
    for annotation in method.annotations:
        annotation_name = get_annotation_name(annotation)
        if annotation_name == 'Query':
            return extract_annotation_value(annotation)
    # Derive query from method name
    return f"Derived query based on method name: {method.name}"

def analyze_documentation(java_class):
    """Check for the presence of JavaDoc comments."""
    documented_methods = 0
    total_methods = 0
    if java_class.methods:
        for method in java_class.methods:
            total_methods += 1
            if method.documentation:
                documented_methods += 1
    coverage = (documented_methods / total_methods) * 100 if total_methods > 0 else 0
    return {
        'documented_methods': documented_methods,
        'total_methods': total_methods,
        'coverage': coverage
    }

def check_dependency_versions(dependencies):
    """Check for outdated dependencies."""
    outdated_dependencies = []
    # Placeholder for actual version checking logic
    for dep, version in dependencies.items():
        # Simulate outdated dependency detection
        if 'SNAPSHOT' in version:
            outdated_dependencies.append({
                'dependency': dep,
                'version': version,
                'issue': 'Snapshot version used'
            })
    return outdated_dependencies

def apply_annotation_filters(annotations: List[str]) -> bool:
    """Check if the class should be scanned based on annotation filters."""
    filters = config['annotation_filters']
    if not filters:
        return True
    return any(annotation in filters for annotation in annotations)

def detect_relationships(java_class, metadata):
    """Detect relationships and filter out non-meaningful ones."""
    relationships = []

    # Inheritance relationships
    if hasattr(java_class, 'extends') and java_class.extends:
        parent_class = java_class.extends.name if isinstance(java_class.extends.name, str) else java_class.extends.name.value
        relationships.append({
            'type': 'inherits',
            'target': parent_class,
            'relationship_type': 'inheritance'
        })

    # Interface implementation relationships
    if java_class.implements:
        for interface in java_class.implements:
            interface_name = interface.name if isinstance(interface.name, str) else interface.name.value
            relationships.append({
                'type': 'implements',
                'target': interface_name,
                'relationship_type': 'interface_implementation'
            })

    # Method call and data flow relationships
    relationships.extend(detect_method_calls(java_class))
    relationships.extend(detect_data_flow(java_class))

    # Entity relationships
    if metadata.get('database_entities'):
        for entity in metadata['database_entities']:
            for rel in entity.get('relationships', []):
                relationships.append({
                    'type': 'entity_relationship',
                    'target': rel['target_entity'],
                    'relationship_type': rel['type']
                })

    # Add import relationships
    if 'imports' in metadata:
        for imp in metadata['imports']:
            relationships.append({
                'type': 'imports',
                'target': imp,
                'relationship_type': 'import'
            })

    # Filter out generic relationships
    meaningful_relationships = []
    for rel in relationships:
        target = rel.get('target', '')
        if target in ['this', 'super', java_class.name]:
            continue  # Skip non-meaningful targets
        if len(target) <= 3:
            continue  # Skip generic or too short targets
        meaningful_relationships.append(rel)

    return meaningful_relationships

def detect_method_calls(java_class):
    """Detect method call relationships within and across services."""
    relationships = []
    if java_class.methods:
        for method in java_class.methods:
            if method.body:
                for path, node in method:
                    if isinstance(node, javalang.tree.MethodInvocation):
                        target = node.qualifier if node.qualifier else 'this'
                        relationships.append({
                            'type': 'calls',
                            'target': target,
                            'relationship_type': 'method_call'
                        })
    return relationships

def detect_data_flow(java_class):
    """Detect data flow across services or between methods."""
    relationships = []
    if java_class.methods:
        for method in java_class.methods:
            if method.body:
                for path, node in method:
                    if isinstance(node, javalang.tree.MemberReference):
                        relationships.append({
                            'type': 'data_flow',
                            'target': node.member,
                            'relationship_type': 'data_access'
                        })
    return relationships

def calculate_methods_complexity(java_class):
    """Calculate cyclomatic complexity of each method in the class."""
    methods_complexity = {}
    if java_class.methods:
        for method in java_class.methods:
            complexity = calculate_cyclomatic_complexity(method)
            methods_complexity[method.name] = complexity
    return methods_complexity

def calculate_cyclomatic_complexity(method):
    """Calculate cyclomatic complexity for a method."""
    complexity = 1  # Start with 1
    for path, node in method:
        if isinstance(node, (javalang.tree.IfStatement, javalang.tree.ForStatement,
                             javalang.tree.WhileStatement, javalang.tree.DoStatement,
                             javalang.tree.SwitchStatement, javalang.tree.TernaryExpression,
                             javalang.tree.CatchClause)):
            complexity += 1
    return complexity

def detect_code_smells(java_class, methods_complexity, is_test_class=False):
    """Detect code smells like long methods and large classes, and Spring Boot specific smells."""
    code_smells = []
    methods_count = len(java_class.methods) if java_class.methods else 0
    fields_count = len(java_class.fields) if java_class.fields else 0

    # Detect large classes
    if methods_count > config.get('class_size_threshold', 10) or fields_count > 10:
        code_smells.append('Large Class')

    # Detect long methods
    method_length_threshold = config.get('method_length_threshold', 50)
    if java_class.methods:
        for method in java_class.methods:
            if method.body and len(method.body) > method_length_threshold:
                code_smells.append(f'Long Method: {method.name}')

    # Detect complex methods
    complexity_threshold = config.get('complexity_threshold', 10)
    for method_name, complexity in methods_complexity.items():
        if complexity > complexity_threshold:
            code_smells.append(f'Complex Method: {method_name} (Cyclomatic Complexity: {complexity})')

    # Detect Spring Boot specific code smells
    if 'RestController' in extract_annotations(java_class) and methods_count > 10:
        code_smells.append('Heavy Controller')

    # Detect misuse of annotations
    annotations = extract_annotations(java_class)
    if 'Service' in annotations and 'Repository' in annotations:
        code_smells.append('Ambiguous Component: Both @Service and @Repository present')

    # Detect God Class
    if methods_count > 20 and fields_count > 20:
        code_smells.append('God Class')

    # Detect Data Class
    if methods_count == 0 and fields_count > 0:
        code_smells.append('Data Class')

    # Detect excessive static methods
    static_methods = sum(1 for method in java_class.methods if 'static' in method.modifiers) if java_class.methods else 0
    if java_class.methods and static_methods > methods_count / 2:
        code_smells.append('Excessive Static Methods')

    if is_test_class:
        code_smells = [smell for smell in code_smells if smell not in ['Large Class', 'God Class']]

    return code_smells

def detect_cyclic_dependencies(scan_summary):
    """Detect cyclic dependencies between classes."""
    dependency_graph = nx.DiGraph()

    # Build dependency graph
    for class_name, metadata in scan_summary.items():
        dependency_graph.add_node(class_name)
        for rel in metadata.get('relationships', []):
            if rel['relationship_type'] in ['method_call', 'data_access']:
                target = rel['target']
                dependency_graph.add_edge(class_name, target)

    # Detect cycles
    cycles = list(nx.simple_cycles(dependency_graph))
    for cycle in cycles:
        logger.warning(f"Cyclic dependency detected: {' -> '.join(cycle)}")
        # Mark classes involved in cycles
        for class_in_cycle in cycle:
            scan_summary[class_in_cycle].setdefault('code_smells', []).append('Cyclic Dependency')

def generate_report(output_format='json', project_path=None, scan_summary=None, output_directory=None):
    """Generate separate reports for code analysis and data analysis."""
    logger.info("Generating scan reports.")

    if output_format == 'json':
        # Use provided output directory or default to project_path
        if output_directory is None:
            output_directory = project_path

        # Ensure the output directory is an absolute path
        output_directory = os.path.abspath(output_directory)

        # Log the output directory
        logger.info(f"Output directory is set to: {output_directory}")

        # Test if we can write to the output directory
        test_file_path = os.path.join(output_directory, 'test_write.txt')
        try:
            with open(test_file_path, 'w') as f:
                f.write('Test write to verify permissions.')
            os.remove(test_file_path)  # Clean up after test
            logger.info("Write test successful.")
        except Exception as e:
            logger.error(f"Cannot write to the specified output directory: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return  # Exit the function if writing is not possible

        code_analysis_file = os.path.join(output_directory, 'code_analysis.json')
        data_analysis_file = os.path.join(output_directory, 'data_analysis.json')

        try:
            # Prepare data for code analysis JSON
            code_analysis_data = {}
            data_analysis_data = {}
            for class_name, metadata in scan_summary.items():
                # Split the metadata into code and data analysis
                code_analysis_metadata = {k: v for k, v in metadata.items() if k in [
                    'annotations', 'methods', 'fields', 'component_type', 'layer',
                    'methods_complexity', 'code_smells', 'relationships'
                ]}

                data_analysis_metadata = {k: v for k, v in metadata.items() if k in [
                    'database_entities', 'config_properties', 'external_api_calls',
                    'messaging_usage', 'api_endpoints', 'bounded_context', 'domain'
                ]}

                # Remove unnecessary fields and fields with empty values
                code_analysis_metadata = clean_metadata(code_analysis_metadata)
                data_analysis_metadata = clean_metadata(data_analysis_metadata)

                # Replace absolute paths with relative paths
                if 'file_path' in code_analysis_metadata:
                    rel_path = os.path.relpath(code_analysis_metadata['file_path'], project_path)
                    code_analysis_metadata['file_path'] = rel_path

                code_analysis_data[class_name] = code_analysis_metadata
                if data_analysis_metadata:
                    data_analysis_data[class_name] = data_analysis_metadata

            # Write code analysis JSON
            with open(code_analysis_file, 'w') as f:
                json.dump(code_analysis_data, f, indent=4)
            logger.info(f"Code analysis JSON written to {code_analysis_file}")

            # Write data analysis JSON
            with open(data_analysis_file, 'w') as f:
                json.dump(data_analysis_data, f, indent=4)
            logger.info(f"Data analysis JSON written to {data_analysis_file}")

        except Exception as e:
            logger.error(f"Error writing JSON reports: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
    else:
        logger.error(f"Unsupported output format or missing project path: {output_format}")

def clean_metadata(metadata):
    """Remove unnecessary fields and fields with empty values."""
    fields_to_remove = ['last_modified', 'is_test_class', 'file_path']
    cleaned_metadata = {}

    for key, value in metadata.items():
        if key in fields_to_remove:
            continue
        if value is None or value == '' or (isinstance(value, list) and not value) or (isinstance(value, dict) and not value):
            continue
        cleaned_metadata[key] = value

    return cleaned_metadata

def detect_bounded_context(java_file_path: Path) -> str:
    """Detect bounded context based on the file path or class annotations."""
    file_path_str = str(java_file_path).lower()

    if "booking" in file_path_str:
        return "Booking Context"
    elif "passenger" in file_path_str:
        return "Passenger Context"
    elif "flight" in file_path_str:
        return "Flight Context"
    else:
        return "Unknown Context"

def extract_api_endpoints(java_class) -> List[str]:
    """Extract API endpoints from a Java class by scanning for annotations like @GetMapping."""
    api_endpoints = []
    class_mappings = []
    # Handle class-level @RequestMapping
    for annotation in java_class.annotations:
        annotation_name = get_annotation_name(annotation)
        if annotation_name == "RequestMapping":
            if annotation.element:
                value = extract_annotation_value(annotation)
                class_mappings.extend(value)

    if java_class.methods:
        for method in java_class.methods:
            method_mappings = []
            http_methods = []
            for annotation in method.annotations:
                annotation_name = get_annotation_name(annotation)
                if annotation_name in {"RequestMapping", "GetMapping", "PostMapping", "PutMapping", "DeleteMapping", "PatchMapping"}:
                    if annotation.element:
                        value = extract_annotation_value(annotation)
                        method_mappings.extend(value)
                    else:
                        method_mappings.append('/')
                    http_methods.append(annotation_name.replace('Mapping', '').upper() if 'Mapping' in annotation_name else annotation_name.upper())
            # Combine class-level and method-level mappings
            for class_mapping in class_mappings or ['/']:
                for method_mapping in method_mappings or ['/']:
                    full_mapping = (class_mapping.rstrip('/') + '/' + method_mapping.lstrip('/')).replace('//', '/')
                    endpoint_info = f"{','.join(http_methods)} {full_mapping}"
                    api_endpoints.append(endpoint_info)

    return api_endpoints

def extract_annotation_value(annotation):
    """Extract the value from an annotation."""
    values = []
    if annotation.element:
        if isinstance(annotation.element, javalang.tree.Literal):
            values.append(annotation.element.value.strip('"\''))
        elif isinstance(annotation.element, javalang.tree.ElementValuePair):
            if isinstance(annotation.element.value, javalang.tree.Literal):
                values.append(annotation.element.value.value.strip('"\''))
        elif isinstance(annotation.element, javalang.tree.ElementArrayValue):
            for element in annotation.element.values:
                if isinstance(element, javalang.tree.Literal):
                    values.append(element.value.strip('"\''))
        elif isinstance(annotation.element, list):
            for elem in annotation.element:
                if isinstance(elem, javalang.tree.ElementValuePair):
                    if isinstance(elem.value, javalang.tree.Literal):
                        values.append(elem.value.value.strip('"\''))
    return values

def extract_domain_from_path(java_file_path: Path) -> str:
    """Extract domain from the file path. This assumes domains are structured in directories."""
    path_parts = str(java_file_path).split(os.sep)

    if "booking" in path_parts:
        return "Booking Domain"
    elif "flight" in path_parts:
        return "Flight Domain"
    elif "passenger" in path_parts:
        return "Passenger Domain"
    else:
        return "Unknown Domain"

def extract_database_entities(java_class):
    """Extract database entity information and relationships."""
    entities = []
    class_annotations = extract_annotations(java_class)
    if 'Entity' in class_annotations:
        entity_info = {
            'table_name': None,
            'fields': [],
            'relationships': []
        }
        # Extract table name
        for annotation in java_class.annotations:
            if get_annotation_name(annotation) == 'Table':
                values = extract_annotation_value(annotation)
                if values:
                    entity_info['table_name'] = values[0]
        # Extract fields and column mappings
        if java_class.fields:
            for field in java_class.fields:
                field_annotations = [get_annotation_name(a) for a in field.annotations]
                column_name = None
                for annotation in field.annotations:
                    if get_annotation_name(annotation) == 'Column':
                        column_values = extract_annotation_value(annotation)
                        if column_values:
                            column_name = column_values[0]
                field_info = {
                    'name': field.declarators[0].name,
                    'type': field.type.name if field.type else 'Unknown',
                    'column_name': column_name,
                    'annotations': field_annotations
                }
                entity_info['fields'].append(field_info)
                # Detect relationships
                relationship = detect_entity_relationships(field)
                if relationship:
                    entity_info['relationships'].append(relationship)
        entities.append(entity_info)
    return entities

def detect_entity_relationships(field):
    """Detect entity relationships like OneToMany, ManyToOne, etc."""
    relationship_types = ['OneToMany', 'ManyToOne', 'ManyToMany', 'OneToOne']
    for annotation in field.annotations:
        annotation_name = get_annotation_name(annotation)
        if annotation_name in relationship_types:
            target_entity = None
            mapped_by = None
            # Extract target entity and mappedBy if available
            if annotation.element:
                if isinstance(annotation.element, javalang.tree.ElementValuePair):
                    if annotation.element.name == 'mappedBy':
                        mapped_by = annotation.element.value.value.strip('"\'')
                elif isinstance(annotation.element, list):
                    for elem in annotation.element:
                        if elem.name == 'mappedBy':
                            mapped_by = elem.value.value.strip('"\'')
                        elif elem.name == 'targetEntity':
                            target_entity = elem.value.value.strip('"\'')
            return {
                'type': annotation_name,
                'field': field.declarators[0].name,
                'target_entity': target_entity,
                'mapped_by': mapped_by
            }
    return None

def detect_messaging_usage(java_class):
    """Detect usage of messaging systems like Kafka or RabbitMQ."""
    messaging_usage = []
    if java_class.fields:
        for field in java_class.fields:
            field_type = field.type.name if field.type else ''
            if field_type in ['KafkaTemplate', 'AmqpTemplate']:
                messaging_usage.append({
                    'type': field_type,
                    'usage': 'Producer'
                })
    if java_class.methods:
        for method in java_class.methods:
            if method.body:
                for path, node in method:
                    if isinstance(node, javalang.tree.MethodInvocation):
                        if node.member in ['send', 'convertAndSend']:
                            messaging_usage.append({
                                'method': method.name,
                                'call': node.member,
                                'usage': 'Messaging Send'
                            })
    return messaging_usage

def detect_external_api_calls(java_class):
    """Detect external API calls using RestTemplate, WebClient, FeignClient, etc."""
    external_api_calls = []
    if java_class.fields:
        for field in java_class.fields:
            field_type = field.type.name if field.type else ''
            if 'FeignClient' in [get_annotation_name(a) for a in field.annotations]:
                external_api_calls.append({
                    'field': field.declarators[0].name,
                    'type': field_type,
                    'usage': 'Feign Client'
                })
    if java_class.methods:
        for method in java_class.methods:
            if method.body:
                for path, node in method:
                    if isinstance(node, javalang.tree.MethodInvocation):
                        if node.qualifier and node.qualifier in ['restTemplate', 'webClient']:
                            external_api_calls.append({
                                'method': method.name,
                                'call': node.member,
                                'usage': 'External API Call',
                                'target': node.arguments[0].value if node.arguments else 'Unknown'
                            })
    return external_api_calls

def extract_config_properties(java_class):
    """Extract configuration properties used in the class."""
    config_properties = []
    if java_class.fields:
        for field in java_class.fields:
            if 'Value' in [get_annotation_name(a) for a in field.annotations]:
                for annotation in field.annotations:
                    if get_annotation_name(annotation) == 'Value':
                        values = extract_annotation_value(annotation)
                        if values:
                            config_properties.append({
                                'field': field.declarators[0].name,
                                'property': values[0]
                            })
    return config_properties

def detect_layer(java_file_path: Path) -> str:
    """Detect the architectural layer of the class based on its file path."""
    file_path_str = str(java_file_path).lower()
    if '/controller/' in file_path_str or file_path_str.endswith('controller.java'):
        return 'Controller Layer'
    elif '/service/' in file_path_str or file_path_str.endswith('service.java'):
        return 'Service Layer'
    elif '/repository/' in file_path_str or file_path_str.endswith('repository.java'):
        return 'Repository Layer'
    elif '/entity/' in file_path_str or file_path_str.endswith('entity.java'):
        return 'Entity Layer'
    else:
        return 'Unknown Layer'

def detect_transactions(java_class):
    """Detect usage of transactions in methods."""
    transactions = []
    if java_class.methods:
        for method in java_class.methods:
            if 'Transactional' in [get_annotation_name(a) for a in method.annotations]:
                transactions.append({
                    'method': method.name,
                    'transactional': True
                })
    return transactions

def detect_global_state(java_class):
    """Detect usage of global state or static variables."""
    global_state_usage = []
    if java_class.fields:
        for field in java_class.fields:
            if 'static' in field.modifiers:
                global_state_usage.append({
                    'field': field.declarators[0].name,
                    'type': field.type.name if field.type else 'Unknown'
                })
    return global_state_usage

def analyze_module_coupling(java_class):
    """Analyze coupling between different modules or packages."""
    coupling = []
    if java_class.methods:
        for method in java_class.methods:
            if method.body:
                for path, node in method:
                    if isinstance(node, javalang.tree.MethodInvocation):
                        if node.qualifier and '.' in node.qualifier:
                            package_name = '.'.join(node.qualifier.split('.')[:-1])
                            coupling.append({
                                'method': method.name,
                                'coupled_module': package_name
                            })
    return coupling

@app.post("/scan_project")
async def scan_project_endpoint(
    project_zip: UploadFile = File(...),
    config_file: Optional[UploadFile] = File(None),
    complexity_threshold: int = Form(10),
    method_length_threshold: int = Form(50),
    class_size_threshold: int = Form(10),
    output_format: str = Form('json'),
    output_directory: Optional[str] = Form(None)
):
    if not project_zip:
        return JSONResponse(status_code=400, content={"error": "Please provide a ZIP file of the Java project."})

    # Save the uploaded project ZIP file
    with tempfile.TemporaryDirectory() as temp_dir:
        project_path = temp_dir

        # Handle ZIP file upload
        if project_zip:
            if project_zip.content_type != 'application/zip':
                return JSONResponse(status_code=400, content={"error": "Invalid file type. Please upload a ZIP file."})

            project_zip_path = os.path.join(temp_dir, project_zip.filename)
            try:
                with open(project_zip_path, 'wb') as f:
                    while True:
                        contents = await project_zip.read(1024)
                        if not contents:
                            break
                        f.write(contents)
            except Exception as e:
                logger.error(f"Error saving uploaded file: {e}")
                return JSONResponse(status_code=500, content={"error": "Failed to save uploaded file."})

            # Check the size of the saved file
            file_size = os.path.getsize(project_zip_path)
            if file_size > MAX_UPLOAD_SIZE:
                return JSONResponse(status_code=400, content={"error": f"File size exceeds the maximum allowed size of {MAX_UPLOAD_SIZE / (1024 * 1024)} MB."})

            # Extract the uploaded ZIP file
            try:
                with zipfile.ZipFile(project_zip_path, 'r') as zip_ref:
                    zip_ref.extractall(temp_dir)
                logger.info("Project uploaded and extracted successfully!")
            except zipfile.BadZipFile:
                logger.error("Invalid ZIP file.")
                return JSONResponse(status_code=400, content={"error": "Invalid ZIP file."})

        # Load configuration if provided
        if config_file:
            config_file_path = os.path.join(temp_dir, config_file.filename)
            try:
                with open(config_file_path, 'wb') as f:
                    while True:
                        contents = await config_file.read(1024)
                        if not contents:
                            break
                        f.write(contents)
            except Exception as e:
                logger.error(f"Error saving configuration file: {e}")
                return JSONResponse(status_code=500, content={"error": "Failed to save configuration file."})

            # Load configuration from the saved file
            with open(config_file_path, 'r') as f:
                config_content = f.read()
            load_configuration(config_content)
        else:
            logger.warning("Using default configuration parameters.")
            config.update({
                'complexity_threshold': complexity_threshold,
                'method_length_threshold': method_length_threshold,
                'class_size_threshold': class_size_threshold
            })

        # Start scanning the project
        scan_summary = scan_project_async(project_path, output_format, output_directory)

        # If there was an error, return it
        if "error" in scan_summary:
            return JSONResponse(status_code=400, content=scan_summary)

        # Generate architecture prompt
        architecture_prompt = generate_architecture_prompt(scan_summary)

        # Return the scan results and the architecture prompt
        return JSONResponse(content={
            "scan_summary": "Scan completed successfully. Check the generated JSON files.",
            "architecture_prompt": architecture_prompt
        })

def generate_architecture_prompt(scan_summary_data: Dict[str, Any]) -> str:
    """Generate a condensed prompt text from the scan summary of the Java project, respecting an 8000-token limit."""
    prompt_lines = []
    total_token_count = 0  # To track token usage

    prompt_lines.append("### Enhanced Java Spring Boot Project Architecture Overview ###\n\n")
    total_token_count += count_tokens(prompt_lines[-1])

    for class_name, metadata in scan_summary_data.items():
        # Add token checks to ensure we stay within limits
        if total_token_count >= MAX_TOKENS:
            prompt_lines.append("\n...Output truncated to stay within token limit...\n")
            break

        prompt_lines.append(f"Class: {class_name}\n")
        total_token_count += count_tokens(prompt_lines[-1])

        prompt_lines.append(f"Component Type: {metadata.get('component_type', 'Unknown')}\n")
        total_token_count += count_tokens(prompt_lines[-1])

        prompt_lines.append(f"Layer: {metadata.get('layer', 'Unknown')}\n")
        total_token_count += count_tokens(prompt_lines[-1])

        prompt_lines.append(f"Bounded Context: {metadata.get('bounded_context', 'Unknown')}\n")
        total_token_count += count_tokens(prompt_lines[-1])

        # Write Annotations
        if metadata.get('annotations'):
            annotations = ', '.join(metadata['annotations'][:3])
            prompt_lines.append(f"Annotations: {annotations}\n")
            total_token_count += count_tokens(prompt_lines[-1])
        else:
            prompt_lines.append("Annotations: None\n")
            total_token_count += count_tokens(prompt_lines[-1])

        # Write Code Smells
        if metadata.get('code_smells'):
            code_smells = ', '.join(metadata['code_smells'])
            prompt_lines.append(f"Code Smells: {code_smells}\n")
            total_token_count += count_tokens(prompt_lines[-1])
        else:
            prompt_lines.append("Code Smells: None\n")
            total_token_count += count_tokens(prompt_lines[-1])

        # Write Relationships
        if metadata.get('relationships'):
            prompt_lines.append("Relationships:\n")
            for rel in metadata['relationships']:
                if rel['relationship_type'] == 'import':
                    prompt_lines.append(f"  - Imports {rel['target']}\n")
                else:
                    prompt_lines.append(f"  - {rel['type']} {rel['target']} ({rel['relationship_type']})\n")
                total_token_count += count_tokens(prompt_lines[-1])

        prompt_lines.append("\n")
        total_token_count += count_tokens(prompt_lines[-1])

    prompt_text = ''.join(prompt_lines)
    logger.info("Architecture prompt generated.")
    return prompt_text
