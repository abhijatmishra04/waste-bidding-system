import streamlit as st
import logging
import os
import json
import yaml
import networkx as nx
import javalang
from groq import Groq
from pathlib import Path
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from pyvis.network import Network
from matplotlib import pyplot as plt
from typing import List, Dict, Any

# Set up logging
logging.basicConfig(
    filename='groq_api_log.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

MAX_TOKENS = 8000  # Define the token limit
scan_summary = defaultdict(lambda: defaultdict(list))  # Store scan results
metadata_cache = {}  # Cache to store metadata
config = {}  # Global config variable

# Streamlit session state initialization
if 'stored_json_content' not in st.session_state:
    st.session_state.stored_json_content = ""
if 'scan_results' not in st.session_state:
    st.session_state.scan_results = {}

# Function to count tokens roughly
def count_tokens(text):
    return len(text) / 4

# Function to generate architecture prompt from JSON
def generate_architecture_prompt(json_data):
    prompt_text = "### Java Project Architecture Overview ###\n\n"
    total_token_count = count_tokens(prompt_text)

    for class_name, metadata in json_data.items():
        if total_token_count >= MAX_TOKENS:
            prompt_text += "\n...Output truncated to stay within token limit...\n"
            break

        prompt_text += f"Class: {class_name}\n"
        total_token_count += count_tokens(f"Class: {class_name}\n")

        prompt_text += f"File Path: {metadata['file_path']}\n"
        total_token_count += count_tokens(f"File Path: {metadata['file_path']}\n")

        prompt_text += f"Component Type: {metadata['component_type']}\n"
        total_token_count += count_tokens(f"Component Type: {metadata['component_type']}\n")

        prompt_text += "Fields:\n"
        for field in metadata['fields'][:3]:
            field_names = ', '.join(field['names'])
            prompt_text += f"  - {field_names} : {field['type']} (Annotations: {', '.join(field['annotations']) if field['annotations'] else 'None'})\n"
            total_token_count += count_tokens(f"  - {field_names} : {field['type']}\n")

        if len(metadata['fields']) > 3:
            prompt_text += f"  ...and {len(metadata['fields']) - 3} more fields\n"
            total_token_count += count_tokens(f"  ...and {len(metadata['fields']) - 3} more fields\n")

        prompt_text += "Methods:\n"
        for method in metadata['methods'][:3]:
            param_list = ', '.join([f"{param['type']} {param['name']}" for param in method['parameters']])
            prompt_text += f"  - {method['return_type']} {method['name']}({param_list})\n"
            total_token_count += count_tokens(f"  - {method['return_type']} {method['name']}\n")

        if len(metadata['methods']) > 3:
            prompt_text += f"  ...and {len(metadata['methods']) - 3} more methods\n"
            total_token_count += count_tokens(f"  ...and {len(metadata['methods']) - 3} more methods\n")

        prompt_text += "\n"
        total_token_count += count_tokens("\n")

    return prompt_text

# Function to load YAML configuration from an uploaded file
def load_configuration_from_file(uploaded_file):
    global config
    try:
        config = yaml.safe_load(uploaded_file)
        logger.info("Successfully loaded configuration from uploaded file")
    except Exception as e:
        logger.error(f"Error loading configuration from uploaded file: {e}")
        config = {
            'annotation_filters': [],  # Set default configuration if loading fails
            'dependency_filters': {'include': [], 'exclude': []},
            'file_path_filters': {'include': [], 'exclude': []}
        }

# Asynchronous project scan
def scan_project_async(project_path: str, output_format: str = 'json'):
    logger.info(f"Starting to scan project at {project_path}")
    java_files = list(Path(project_path).rglob("*.java"))

    if not java_files:
        logger.error(f"No Java files found in the directory: {project_path}. Exiting.")
        return

    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
        futures = []
        for java_file in java_files:
            futures.append(executor.submit(scan_file, java_file))

        for future in as_completed(futures):
            try:
                future.result()
            except Exception as exc:
                logger.error(f"Batch generated an exception: {exc}")

    generate_report(output_format, project_path)

# Function to scan individual Java files
def scan_file(java_file_path: Path):
    try:
        with open(java_file_path, 'r', encoding='utf-8') as file:
            java_code = file.read()

        tree = javalang.parse.parse(java_code)

        for node in tree.types:
            if isinstance(node, javalang.tree.ClassDeclaration):
                scan_class(node, java_file_path)

    except Exception as e:
        logger.error(f"Error parsing {java_file_path}: {e}")

# Function to scan a Java class and extract metadata
def scan_class(java_class, java_file_path: Path):
    class_name = java_class.name
    last_modified = os.path.getmtime(java_file_path)

    if class_name in metadata_cache and metadata_cache[class_name]['last_modified'] >= last_modified:
        logger.info(f"Skipping cached class {class_name}")
        return

    class_annotations = extract_annotations(java_class)
    if not apply_annotation_filters(class_annotations):
        logger.info(f"Class {class_name} filtered out by annotation filter")
        return

    metadata = {
        'annotations': class_annotations,
        'dependencies': [],
        'inter_service_calls': [],
        'fields': [],
        'methods': [],
        'relationships': [],
        'file_path': str(java_file_path),
        'last_modified': last_modified,
        'component_type': detect_spring_component_type(java_class)
    }

    metadata['fields'] = extract_fields(java_class)
    metadata['methods'] = extract_methods(java_class)

    detect_service_injections(java_class, metadata)

    metadata_cache[class_name] = metadata
    scan_summary[class_name] = metadata
    scan_summary[class_name]['relationships'] = detect_relationships(java_class, metadata)

    logger.info(f"Class {class_name} scanned with metadata: {metadata}")

# Extract imports from a Java class
def extract_imports(tree):
    imports = []
    if hasattr(tree, 'imports'):
        for imp in tree.imports:
            imports.append(imp.path)
    return imports

# Detect Spring component type (Service, Controller, etc.)
def detect_spring_component_type(java_class):
    component_types = ["RestController", "Controller", "Service", "Repository", "Component"]

    for annotation in java_class.annotations:
        annotation_name = annotation.name if isinstance(annotation.name, str) else annotation.name.value
        if annotation_name in component_types:
            return annotation_name

    return "Unknown"

# Calculate complexity metrics such as cyclomatic complexity, class size, etc.
def calculate_complexity_metrics(java_class):
    complexity = {
        'class_size': len(java_class.fields) + len(java_class.methods),
        'method_complexity': {},
        'inheritance_depth': calculate_inheritance_depth(java_class),
    }

    for method in java_class.methods:
        complexity['method_complexity'][method.name] = calculate_cyclomatic_complexity(method)

    return complexity

# Detect relationships within classes: inheritance, method calls, data flow
def detect_relationships(java_class, metadata):
    relationships = []

    if hasattr(java_class, 'extends') and java_class.extends:
        parent_class = java_class.extends.name if isinstance(java_class.extends.name,
                                                             str) else java_class.extends.name.value
        relationships.append({
            'type': 'inherits',
            'target': parent_class,
            'relationship_type': 'inheritance'
        })

    if hasattr(java_class, 'implements') and java_class.implements:
        for interface in java_class.implements:
            interface_name = interface.name if isinstance(interface.name, str) else interface.name.value
            relationships.append({
                'type': 'implements',
                'target': interface_name,
                'relationship_type': 'interface_implementation'
            })

    method_call_relationships = detect_method_calls(java_class)
    relationships.extend(method_call_relationships)

    return relationships

# Detect method calls within a class
def detect_method_calls(java_class):
    """Detect method call relationships within and across services."""
    relationships = []
    for method in java_class.methods:
        if method.body:
            for node in method.body:
                if isinstance(node, javalang.tree.MethodInvocation):
                    relationships.append({
                        'type': 'calls',
                        'target': node.member or '',  # Ensure target is not None
                        'relationship_type': 'method_call'
                    })
    return relationships

# Extract annotations from a Java class
def extract_annotations(java_class):
    """Extract annotations from a Java class."""
    return [annotation.name if isinstance(annotation.name, str) else str(annotation.name.value or '') for annotation in java_class.annotations]

# Extract fields from a Java class
def extract_fields(java_class):
    """Extract fields and their metadata from a Java class."""
    return [{
        'names': [declarator.name for declarator in field.declarators],
        'type': field.type.name if field.type and isinstance(field.type.name, str) else 'Unknown',
        'annotations': [annotation.name if isinstance(annotation.name, str) else str(annotation.name.value or '') for annotation in field.annotations]
    } for field in java_class.fields]

# Extract methods from a Java class
def extract_methods(java_class):
    """Extract methods from a Java class."""
    return [{
        'name': method.name or '',  # Ensure method name is not None
        'return_type': method.return_type.name if method.return_type and isinstance(method.return_type.name, str) else 'void',
        'parameters': [{'name': param.name or '',  # Ensure parameter name is not None
                        'type': param.type.name if param.type and isinstance(param.type.name, str) else 'Unknown'} for param in method.parameters],
        'annotations': [annotation.name if isinstance(annotation.name, str) else str(annotation.name.value or '') for annotation in method.annotations]
    } for method in java_class.methods]


# Detect service injections in fields or constructor-based injection
def detect_service_injections(java_class, metadata):
    for field in java_class.fields:
        if field.annotations:
            for annotation in field.annotations:
                annotation_name = annotation.name if isinstance(annotation.name, str) else annotation.name.value
                if annotation_name == "Autowired":
                    injected_service = field.type.name if field.type and field.type.name else 'Unknown'
                    metadata['inter_service_calls'].append(injected_service)
                    logger.info(f"Autowired service found in {java_class.name}: {injected_service}")

    for constructor in java_class.constructors:
        for param in constructor.parameters:
            for annotation in param.annotations:
                annotation_name = annotation.name if isinstance(annotation.name, str) else annotation.name.value
                if annotation_name == "Autowired":
                    injected_service = param.type.name if param.type and param.type.name else 'Unknown'
                    metadata['inter_service_calls'].append(injected_service)
                    logger.info(f"Autowired service found in constructor of {java_class.name}: {injected_service}")

# Apply annotation filters to a class (used in scanning)
def apply_annotation_filters(annotations: List[str]) -> bool:
    filters = config['annotation_filters']
    if not filters:
        return True
    return any(annotation in filters for annotation in annotations)

# Generate report (JSON or HTML) for the scan
def generate_report(output_format='json', project_path=None):
    if output_format == 'json':
        summary_file = os.path.join(project_path, 'scan_summary.json')
        with open(summary_file, 'w') as f:
            json.dump(scan_summary, f, indent=4)
        logger.info(f"JSON scan summary written to {summary_file}")

# Visualize inter-service dependencies using networkx
def visualize_dependencies(scan_summary):
    G = nx.DiGraph()
    for class_name, metadata in scan_summary.items():
        G.add_node(class_name, label=metadata['component_type'])
        for rel in metadata['relationships']:
            G.add_edge(class_name, rel['target'], label=rel['relationship_type'])

    pos = nx.spring_layout(G, k=0.5, iterations=50)

    plt.figure(figsize=(12, 12))
    nx.draw_networkx_nodes(G, pos, node_color="skyblue", node_size=3000)
    nx.draw_networkx_edges(G, pos, arrowstyle="->", arrowsize=20, edge_color="gray")
    nx.draw_networkx_labels(G, pos, font_size=12, font_family="sans-serif")

    edge_labels = nx.get_edge_attributes(G, "label")
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)

    plt.title("Inter-Service Dependencies")
    st.pyplot(plt.gcf())  # Render with Streamlit

# Main Streamlit UI
st.title("GEMS-MonoToMicro Transformation Framework")

# User choice for operation: Scan project or upload JSON
operation = st.selectbox("Choose operation:", ["Upload JSON for Prompt", "Scan Java Project"])

if operation == "Upload JSON for Prompt":
    prompt = st.text_area("Enter your prompt:", "")
    uploaded_file = st.file_uploader("Upload a JSON file", type="json")

    if st.button("Send"):
        if uploaded_file:
            json_data = json.load(uploaded_file)
            generated_prompt = generate_architecture_prompt(json_data)
            st.session_state.stored_json_content = generated_prompt

            # Groq API call
            try:
                combined_prompt = f"{prompt}\n following is the json: {generated_prompt}"
                logging.info(f"User prompt: {combined_prompt}")

                # Initialize Groq client
                client = Groq(api_key="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")
                completion = client.chat.completions.create(
                    model="llama3-8b-8192",
                    messages=[{"role": "user", "content": combined_prompt}],
                    temperature=1,
                    max_tokens=8000,
                    top_p=1,
                    stream=True,
                )

                response_text = "".join([chunk.choices[0].delta.content for chunk in completion])
                st.write("Response:", response_text)

            except Exception as e:
                st.error(f"Error: {e}")

elif operation == "Scan Java Project":
    project_path = st.text_input("Enter the path to the Java project:")
    uploaded_config_file = st.file_uploader("Upload configuration file (config.yaml)", type="yaml")
    output_format = st.selectbox("Select output format:", ['json', 'html'])

    if st.button("Start Scan"):
        if project_path:
            # Load uploaded configuration file
            if uploaded_config_file is not None:
                load_configuration_from_file(uploaded_config_file)
            else:
                st.error("Please upload a configuration file before scanning.")

            # Start scanning
            scan_project_async(project_path, output_format)

            # Visualize scan results
            if scan_summary:
                visualize_dependencies(scan_summary)
