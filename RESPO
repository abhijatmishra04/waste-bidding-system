import os
import zipfile
import tempfile
import javalang
import yaml
import logging
import json
import networkx as nx
from pathlib import Path
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import JSONResponse
import re

# Load environment variables
load_dotenv()

# Set up the logger with enhanced formatting and handlers
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
# Create handlers
file_handler = logging.FileHandler('groq_api_log.log')
stream_handler = logging.StreamHandler()
# Create formatters and add to handlers
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)
stream_handler.setFormatter(formatter)
# Add handlers to the logger
if not logger.handlers:
    logger.addHandler(file_handler)
    logger.addHandler(stream_handler)

# Configure security settings
ALLOWED_FILE_TYPES = ['zip']
MAX_UPLOAD_SIZE = 50 * 1024 * 1024  # 50 MB

GROQ_API_KEY = "your_groq_api_key_here"
if not GROQ_API_KEY:
    logger.error("Groq API key not found. Please set the GROQ_API_KEY environment variable.")

# Initialize FastAPI app
app = FastAPI()

# Configuration and constants
MAX_TOKENS = 5000000  # Token limit for prompt generation
DEFAULT_CONFIG = {
    'annotation_filters': [],
    'dependency_filters': {'include': [], 'exclude': []},
    'file_path_filters': {'include': [], 'exclude': []},
    'complexity_threshold': 10,
    'method_length_threshold': 50,
    'class_size_threshold': 10,
    'batch_size': 50,
    'output_file': 'scan_summary.json'
}
config = DEFAULT_CONFIG.copy()

def count_tokens(text):
    """Estimate the number of tokens in the text."""
    return len(text) / 4

def load_configuration(config_content):
    """Load configuration from uploaded YAML content."""
    global config
    try:
        user_config = yaml.safe_load(config_content)
        config = {**DEFAULT_CONFIG, **user_config}
        logger.info("Successfully loaded configuration.")
    except yaml.YAMLError:
        logger.exception("Error parsing configuration file.")
        config = DEFAULT_CONFIG.copy()
    except Exception:
        logger.exception("Error loading configuration.")
        config = DEFAULT_CONFIG.copy()

def scan_project_async(project_dir: str, output_format: str = 'json') -> Dict[str, Any]:
    """Scan the Java project asynchronously with filtering options."""
    scan_summary = {}
    metadata_cache = {}
    try:
        logger.info("Starting to scan project at %s", project_dir)

        java_files = list(Path(project_dir).rglob("*.java"))

        if not java_files:
            logger.error("No Java files found in the directory.")
            return {"error": "No Java files found in the project."}

        logger.info("Found %d Java files to scan.", len(java_files))

        # Apply file path filters
        java_files = apply_file_path_filters(java_files)

        batch_size = config.get('batch_size', 50)
        total_files = len(java_files)
        processed_files = 0

        # Parse configuration files
        configurations = parse_configuration_files(project_dir)

        # Parse dependency files
        dependencies = parse_dependency_files(project_dir)

        # Use ThreadPoolExecutor for concurrent scanning
        with ThreadPoolExecutor(max_workers=os.cpu_count() or 1) as executor:
            futures = []
            for batch in [java_files[i:i + batch_size] for i in range(0, len(java_files), batch_size)]:
                futures.append(executor.submit(scan_batch, batch, scan_summary, metadata_cache, configurations, dependencies))

            # Update progress as futures complete
            for future in as_completed(futures):
                try:
                    future.result()
                    processed_files += batch_size
                    progress = processed_files / total_files
                    progress_percent = min(progress, 1.0) * 100
                    logger.info("Progress: %.2f%%", progress_percent)
                except Exception:
                    logger.exception("Batch generated an exception.")

        # Detect cyclic dependencies after scanning all files
        detect_cyclic_dependencies(scan_summary)
        generate_report(output_format, project_dir, scan_summary)
    except Exception:
        logger.exception("Error during project scan.")
        return {"error": "Error during project scan."}

    return scan_summary

def parse_configuration_files(project_dir: str) -> Dict[str, Any]:
    """Parse application.properties and application.yml files."""
    configurations = {}
    # Parse application.properties files
    properties_files = list(Path(project_dir).rglob("application*.properties"))
    for prop_file in properties_files:
        try:
            with open(prop_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                for line in lines:
                    line = line.strip()
                    # Skip comments and empty lines
                    if not line or line.startswith('#') or '=' not in line:
                        continue
                    key, value = line.split('=', 1)
                    configurations[key.strip()] = value.strip()
        except Exception:
            logger.exception(f"Error parsing properties file {prop_file}")

    # Parse application.yml files
    yml_files = list(Path(project_dir).rglob("application*.yml")) + list(Path(project_dir).rglob("application*.yaml"))
    for yml_file in yml_files:
        try:
            with open(yml_file, 'r', encoding='utf-8') as f:
                yml_content = yaml.safe_load(f)
                if yml_content:
                    configurations.update(flatten_dict(yml_content))
        except Exception:
            logger.exception(f"Error parsing YAML file {yml_file}")

    logger.info("Configuration files parsed.")
    return configurations

def flatten_dict(d, parent_key='', sep='.'):
    """Flatten nested dictionaries into a single-level dictionary."""
    items = []
    for k, v in d.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_dict(v, new_key, sep=sep).items())
        else:
            items.append((new_key, v))
    return dict(items)

def parse_dependency_files(project_dir: str) -> Dict[str, Any]:
    """Parse pom.xml or build.gradle files to extract dependencies."""
    dependencies = {}
    # Parse pom.xml files
    pom_files = list(Path(project_dir).rglob("pom.xml"))
    for pom_file in pom_files:
        try:
            with open(pom_file, 'r', encoding='utf-8') as f:
                content = f.read()
                dependency_matches = re.findall(r'<dependency>.*?</dependency>', content, re.DOTALL)
                for dep in dependency_matches:
                    group_id = re.search(r'<groupId>(.*?)</groupId>', dep)
                    artifact_id = re.search(r'<artifactId>(.*?)</artifactId>', dep)
                    version = re.search(r'<version>(.*?)</version>', dep)
                    if group_id and artifact_id and version:
                        key = f"{group_id.group(1)}:{artifact_id.group(1)}"
                        dependencies[key] = version.group(1)
        except Exception:
            logger.exception(f"Error parsing pom.xml file {pom_file}")

    logger.info("Dependency files parsed.")
    return dependencies

def apply_file_path_filters(java_files: List[Path]) -> List[Path]:
    """Apply include/exclude filters to the list of Java files based on file paths."""
    include_filters = config['file_path_filters'].get('include', [])
    exclude_filters = config['file_path_filters'].get('exclude', [])

    filtered_files = []
    for java_file in java_files:
        file_str = str(java_file)
        if include_filters and not any(pattern in file_str for pattern in include_filters):
            continue
        if any(pattern in file_str for pattern in exclude_filters):
            continue
        filtered_files.append(java_file)

    logger.info("After applying filters, %d Java files will be scanned.", len(filtered_files))
    return filtered_files

def scan_batch(file_batch: List[Path], scan_summary, metadata_cache, configurations, dependencies):
    """Scan a batch of Java files."""
    for java_file in file_batch:
        scan_file(java_file, scan_summary, metadata_cache, configurations, dependencies)

def scan_file(java_file_path: Path, scan_summary, metadata_cache, configurations, dependencies):
    """Scan a single Java file for services, annotations, and dependencies."""
    try:
        with open(java_file_path, 'r', encoding='utf-8') as file:
            java_code = file.read()

        tree = javalang.parse.parse(java_code)

        for node in tree.types:
            if isinstance(node, javalang.tree.ClassDeclaration):
                scan_class(node, java_file_path, scan_summary, metadata_cache, configurations, dependencies)

        imports = extract_imports(tree)
        logger.debug("Imports found in %s: %s", java_file_path, imports)

    except javalang.parser.JavaSyntaxError:
        logger.exception(f"Syntax error in {java_file_path}")
    except Exception:
        logger.exception(f"Error parsing {java_file_path}")

def scan_class(java_class, java_file_path: Path, scan_summary, metadata_cache, configurations, dependencies):
    """Scan a Java class for advanced analysis: annotations, complexity, dependencies, and relationships."""
    try:
        class_name = java_class.name
        last_modified = os.path.getmtime(java_file_path)

        if class_name in metadata_cache and metadata_cache[class_name]['last_modified'] >= last_modified:
            logger.info("Skipping cached class %s", class_name)
            return

        class_annotations = extract_annotations(java_class)
        if not apply_annotation_filters(class_annotations):
            logger.info("Class %s filtered out by annotation filter", class_name)
            return

        # Determine if the class is a test class
        is_test_class = 'Test' in class_annotations or java_file_path.parts[-2].lower() == 'test'

        # Extract bounded context, API endpoints, and domain information
        bounded_context = detect_bounded_context(java_file_path)
        api_endpoints = extract_api_endpoints(java_class)
        domain = extract_domain_from_path(java_file_path)

        # Advanced analysis: Calculate cyclomatic complexity and detect code smells
        methods_complexity = calculate_methods_complexity(java_class)
        code_smells = detect_code_smells(java_class, methods_complexity, is_test_class)

        # Collect metadata for the class
        metadata = {
            'annotations': class_annotations,
            'dependencies': [],
            'inter_service_calls': [],
            'fields': extract_fields(java_class),
            'methods': extract_methods(java_class),
            'relationships': [],
            'file_path': str(java_file_path),
            'last_modified': last_modified,
            'component_type': detect_spring_component_type(java_class),
            'bounded_context': bounded_context,
            'api_endpoints': api_endpoints,
            'domain': domain,
            'methods_complexity': methods_complexity,
            'code_smells': code_smells,
            'is_test_class': is_test_class,
            'database_entities': [],
            'messaging_usage': [],
            'external_api_calls': [],
            'config_properties': [],
            'layer': detect_layer(java_file_path),
            'transactions': detect_transactions(java_class),
            'global_state_usage': detect_global_state(java_class),
        }

        # Detect service injections and relationships
        detect_service_injections(java_class, metadata)
        metadata['relationships'] = detect_relationships(java_class, metadata)

        # Extract Spring Beans
        if metadata['component_type'] in ["Configuration", "Component"]:
            beans = extract_spring_beans(java_class)
            metadata['spring_beans'] = beans

        # Security Analysis
        security_info = analyze_security(java_class)
        metadata['security'] = security_info

        # AOP Analysis
        aspects = detect_aspects(java_class)
        if aspects:
            metadata['aspects'] = aspects

        # Profile and Conditional Beans
        profiles = extract_profiles(java_class)
        if profiles:
            metadata['profiles'] = profiles

        # Microservice Communication Analysis
        inter_service_calls = detect_inter_service_calls(java_class)
        if inter_service_calls:
            metadata['microservice_calls'] = inter_service_calls

        # Database Interaction Analysis
        db_interactions = analyze_database_interactions(java_class)
        if db_interactions:
            metadata['database_interactions'] = db_interactions

        # Database Entity Extraction
        database_entities = extract_database_entities(java_class)
        if database_entities:
            metadata['database_entities'] = database_entities

        # Messaging Patterns Detection
        messaging_usage = detect_messaging_usage(java_class)
        if messaging_usage:
            metadata['messaging_usage'] = messaging_usage

        # External API Calls Detection
        external_api_calls = detect_external_api_calls(java_class)
        if external_api_calls:
            metadata['external_api_calls'] = external_api_calls

        # Configuration Properties Extraction
        config_properties = extract_config_properties(java_class)
        if config_properties:
            metadata['config_properties'] = config_properties

        # Module Coupling Analysis
        module_coupling = analyze_module_coupling(java_class)
        if module_coupling:
            metadata['module_coupling'] = module_coupling

        # Documentation Analysis
        documentation_coverage = analyze_documentation(java_class)
        metadata['documentation_coverage'] = documentation_coverage

        # Dependency Version Checks
        outdated_dependencies = check_dependency_versions(dependencies)
        metadata['outdated_dependencies'] = outdated_dependencies

        # Cache and summarize the metadata
        metadata_cache[class_name] = metadata
        scan_summary[class_name] = metadata

        logger.debug("Class %s scanned with metadata.", class_name)
    except Exception:
        logger.exception(f"Error scanning class {java_class.name}")

def extract_imports(tree):
    """Extract imports from the CompilationUnit."""
    imports = []
    if hasattr(tree, 'imports'):
        for imp in tree.imports:
            imports.append(imp.path)
    return imports

def extract_annotations(java_class):
    """Extract annotations from a Java class."""
    annotations = []
    for annotation in java_class.annotations:
        try:
            annotation_name = get_annotation_name(annotation)
            annotations.append(annotation_name)
        except Exception:
            logger.exception("Error extracting annotation name.")
    return annotations

def get_annotation_name(annotation):
    """Get the name of an annotation."""
    if isinstance(annotation.name, str):
        return annotation.name
    elif hasattr(annotation.name, 'qualifier') and annotation.name.qualifier:
        return f"{annotation.name.qualifier}.{annotation.name.member}"
    elif hasattr(annotation.name, 'member'):
        return annotation.name.member
    elif isinstance(annotation.name, (tuple, list)):
        # Handle cases where annotation.name is a tuple or list
        names = []
        for name_part in annotation.name:
            if isinstance(name_part, str):
                names.append(name_part)
            elif hasattr(name_part, 'member'):
                names.append(name_part.member)
            elif hasattr(name_part, 'name'):
                names.append(name_part.name)
        return '.'.join(names)
    else:
        return str(annotation.name)

def apply_annotation_filters(annotations: List[str]) -> bool:
    """Check if the class should be scanned based on annotation filters."""
    filters = config['annotation_filters']
    if not filters:
        return True
    return any(annotation in filters for annotation in annotations)

def extract_fields(java_class):
    """Extract fields and their metadata from a Java class."""
    fields = []
    if java_class.fields:
        for field in java_class.fields:
            try:
                field_type_name = ''
                if field.type:
                    if isinstance(field.type.name, str):
                        field_type_name = field.type.name
                    elif isinstance(field.type.name, (tuple, list)):
                        field_type_name = '.'.join(field.type.name)
                    else:
                        field_type_name = str(field.type.name)
                else:
                    field_type_name = 'Unknown'

                field_info = {
                    'names': [declarator.name for declarator in field.declarators],
                    'type': field_type_name,
                    'annotations': [get_annotation_name(annotation) for annotation in field.annotations],
                    'modifiers': list(field.modifiers)
                }
                fields.append(field_info)
            except Exception:
                logger.exception("Error extracting field information.")
    return fields

def extract_methods(java_class):
    """Extract methods from a Java class."""
    methods = []
    if java_class.methods:
        for method in java_class.methods:
            try:
                return_type_name = ''
                if method.return_type:
                    if isinstance(method.return_type.name, str):
                        return_type_name = method.return_type.name
                    elif isinstance(method.return_type.name, (tuple, list)):
                        return_type_name = '.'.join(method.return_type.name)
                    else:
                        return_type_name = str(method.return_type.name)
                else:
                    return_type_name = 'void'

                method_info = {
                    'name': method.name,
                    'return_type': return_type_name,
                    'parameters': [{
                        'name': param.name,
                        'type': param.type.name if param.type and isinstance(param.type.name, str) else 'Unknown'
                    } for param in method.parameters],
                    'annotations': [get_annotation_name(annotation) for annotation in method.annotations],
                    'modifiers': list(method.modifiers),
                    'body_length': len(method.body) if method.body else 0
                }
                methods.append(method_info)
            except Exception:
                logger.exception("Error extracting method information.")
    return methods

def detect_spring_component_type(java_class):
    """Detect the Spring component type of the Java class based on its annotations."""
    component_types = [
        "SpringBootApplication", "Configuration", "RestController", "Controller",
        "Service", "Repository", "Component"
    ]

    for annotation in java_class.annotations:
        annotation_name = get_annotation_name(annotation)
        if annotation_name in component_types:
            return annotation_name

    return "Unknown"

def detect_service_injections(java_class, metadata):
    """Detect field-level, constructor-based, and method-based service injections."""
    # Field injection
    if java_class.fields:
        for field in java_class.fields:
            if field.annotations:
                for annotation in field.annotations:
                    annotation_name = get_annotation_name(annotation)
                    if annotation_name in ["Autowired", "Inject", "Resource"]:
                        injected_service = field.type.name if field.type and field.type.name else 'Unknown'
                        if isinstance(injected_service, (tuple, list)):
                            injected_service = '.'.join(injected_service)
                        metadata['inter_service_calls'].append(injected_service)
                        logger.debug("Injected service found in %s: %s", java_class.name, injected_service)

    # Constructor injection
    if java_class.constructors:
        for constructor in java_class.constructors:
            if constructor.annotations:
                for annotation in constructor.annotations:
                    annotation_name = get_annotation_name(annotation)
                    if annotation_name in ["Autowired", "Inject"]:
                        for param in constructor.parameters:
                            injected_service = param.type.name if param.type and param.type.name else 'Unknown'
                            if isinstance(injected_service, (tuple, list)):
                                injected_service = '.'.join(injected_service)
                            metadata['inter_service_calls'].append(injected_service)
                            logger.debug("Injected service found in constructor of %s: %s", java_class.name, injected_service)

    # Method injection
    if java_class.methods:
        for method in java_class.methods:
            if method.annotations:
                for annotation in method.annotations:
                    annotation_name = get_annotation_name(annotation)
                    if annotation_name in ["Autowired", "Inject"]:
                        for param in method.parameters:
                            injected_service = param.type.name if param.type and param.type.name else 'Unknown'
                            if isinstance(injected_service, (tuple, list)):
                                injected_service = '.'.join(injected_service)
                            metadata['inter_service_calls'].append(injected_service)
                            logger.debug("Injected service found in method %s of %s: %s", method.name, java_class.name, injected_service)

def detect_relationships(java_class, metadata):
    """Detect advanced relationships: inheritance, interfaces, method calls, cyclic dependencies, data flow, entity relationships."""
    relationships = []

    # Inheritance relationships
    if hasattr(java_class, 'extends') and java_class.extends:
        if isinstance(java_class.extends, (list, tuple)):
            for extend in java_class.extends:
                parent_class = extend.name if isinstance(extend.name, str) else extend.name.value
                relationships.append({
                    'type': 'inherits',
                    'target': parent_class,
                    'relationship_type': 'inheritance'
                })
        else:
            parent_class = java_class.extends.name if isinstance(java_class.extends.name, str) else java_class.extends.name.value
            relationships.append({
                'type': 'inherits',
                'target': parent_class,
                'relationship_type': 'inheritance'
            })

    # Interface implementation relationships
    if java_class.implements:
        # Ensure implements is iterable
        implements = java_class.implements if isinstance(java_class.implements, (list, tuple)) else [java_class.implements]
        for interface in implements:
            interface_name = interface.name if isinstance(interface.name, str) else interface.name.value
            relationships.append({
                'type': 'implements',
                'target': interface_name,
                'relationship_type': 'interface_implementation'
            })

    # Method call and data flow relationships
    relationships.extend(detect_method_calls(java_class))
    relationships.extend(detect_data_flow(java_class))

    # Entity relationships
    if metadata.get('database_entities'):
        for entity in metadata['database_entities']:
            for rel in entity.get('relationships', []):
                relationships.append({
                    'type': 'entity_relationship',
                    'target': rel['target_entity'],
                    'relationship_type': rel['type']
                })

    return relationships

def detect_method_calls(java_class):
    """Detect method call relationships within and across services."""
    relationships = []
    if java_class.methods:
        for method in java_class.methods:
            if method.body:
                for path, node in method:
                    if isinstance(node, javalang.tree.MethodInvocation):
                        target = node.qualifier if node.qualifier else 'this'
                        relationships.append({
                            'type': 'calls',
                            'target': target,
                            'relationship_type': 'method_call'
                        })
    return relationships

def detect_data_flow(java_class):
    """Detect data flow across services or between methods."""
    relationships = []
    if java_class.methods:
        for method in java_class.methods:
            if method.body:
                for path, node in method:
                    if isinstance(node, javalang.tree.MemberReference):
                        relationships.append({
                            'type': 'data_flow',
                            'target': node.member,
                            'relationship_type': 'data_access'
                        })
    return relationships

def calculate_methods_complexity(java_class):
    """Calculate cyclomatic complexity of each method in the class."""
    methods_complexity = {}
    if java_class.methods:
        for method in java_class.methods:
            complexity = calculate_cyclomatic_complexity(method)
            methods_complexity[method.name] = complexity
    return methods_complexity

def calculate_cyclomatic_complexity(method):
    """Calculate cyclomatic complexity for a method."""
    complexity = 1  # Start with 1
    for path, node in method:
        if isinstance(node, (javalang.tree.IfStatement, javalang.tree.ForStatement,
                             javalang.tree.WhileStatement, javalang.tree.DoStatement,
                             javalang.tree.SwitchStatement, javalang.tree.TernaryExpression,
                             javalang.tree.CatchClause)):
            complexity += 1
    return complexity

def detect_code_smells(java_class, methods_complexity, is_test_class=False):
    """Detect code smells like long methods and large classes, and Spring Boot specific smells."""
    code_smells = []
    methods_count = len(java_class.methods) if java_class.methods else 0
    fields_count = len(java_class.fields) if java_class.fields else 0

    # Detect large classes
    if methods_count > config.get('class_size_threshold', 10) or fields_count > 10:
        code_smells.append('Large Class')

    # Detect long methods
    method_length_threshold = config.get('method_length_threshold', 50)
    if java_class.methods:
        for method in java_class.methods:
            if method.body and len(method.body) > method_length_threshold:
                code_smells.append(f'Long Method: {method.name}')

    # Detect complex methods
    complexity_threshold = config.get('complexity_threshold', 10)
    for method_name, complexity in methods_complexity.items():
        if complexity > complexity_threshold:
            code_smells.append(f'Complex Method: {method_name} (Cyclomatic Complexity: {complexity})')

    # Detect Spring Boot specific code smells
    if 'RestController' in extract_annotations(java_class) and methods_count > 10:
        code_smells.append('Heavy Controller')

    # Detect misuse of annotations
    annotations = extract_annotations(java_class)
    if 'Service' in annotations and 'Repository' in annotations:
        code_smells.append('Ambiguous Component: Both @Service and @Repository present')

    # Detect God Class
    if methods_count > 20 and fields_count > 20:
        code_smells.append('God Class')

    # Detect Data Class
    if methods_count == 0 and fields_count > 0:
        code_smells.append('Data Class')

    # Detect excessive static methods
    static_methods = sum(1 for method in java_class.methods if 'static' in method.modifiers) if java_class.methods else 0
    if java_class.methods and static_methods > methods_count / 2:
        code_smells.append('Excessive Static Methods')

    if is_test_class:
        code_smells = [smell for smell in code_smells if smell not in ['Large Class', 'God Class']]

    return code_smells

def detect_cyclic_dependencies(scan_summary):
    """Detect cyclic dependencies between classes."""
    dependency_graph = nx.DiGraph()

    # Build dependency graph
    for class_name, metadata in scan_summary.items():
        dependency_graph.add_node(class_name)
        for rel in metadata.get('relationships', []):
            if rel['relationship_type'] in ['method_call', 'data_access']:
                target = rel['target']
                dependency_graph.add_edge(class_name, target)

    # Detect cycles
    cycles = list(nx.simple_cycles(dependency_graph))
    for cycle in cycles:
        logger.warning("Cyclic dependency detected: %s", ' -> '.join(cycle))
        # Mark classes involved in cycles
        for class_in_cycle in cycle:
            scan_summary[class_in_cycle].setdefault('code_smells', []).append('Cyclic Dependency')

def generate_report(output_format='json', project_path=None, scan_summary=None):
    """Generate a report of the scan in the desired format (JSON, HTML)."""
    logger.info("Generating scan report.")

    if output_format == 'json' and project_path:
        summary_file = os.path.join(project_path, config.get('output_file', 'scan_summary.json'))
        try:
            with open(summary_file, 'w') as f:
                json.dump(scan_summary, f, indent=4)
            logger.info("JSON scan summary written to %s", summary_file)
        except Exception:
            logger.exception("Error writing JSON report.")
    elif output_format == 'html' and project_path:
        generate_html_report(project_path, scan_summary)
    else:
        logger.error("Unsupported output format or missing project path: %s", output_format)

def generate_html_report(project_path=None, scan_summary=None):
    """Generate an HTML report from the scan summary."""
    summary_file = os.path.join(project_path, config.get('output_file', 'scan_summary.html'))
    try:
        with open(summary_file, 'w') as f:
            f.write("<html><head><title>Scan Report</title></head><body>")
            f.write("<h1>Scan Report</h1>")
            for class_name, data in scan_summary.items():
                f.write(f"<h2>{class_name}</h2>")
                f.write(f"<p>File: {data['file_path']}</p>")
                f.write(f"<p>Component Type: {data['component_type']}</p>")
                f.write(f"<p>Layer: {data.get('layer', 'Unknown')}</p>")
                f.write(f"<p>Bounded Context: {data['bounded_context']}</p>")
                f.write("<h3>Annotations</h3><ul>")
                for annotation in data['annotations']:
                    f.write(f"<li>{annotation}</li>")
                f.write("</ul>")
                f.write("<h3>Code Smells</h3><ul>")
                for smell in data.get('code_smells', []):
                    f.write(f"<li>{smell}</li>")
                f.write("</ul>")
                f.write("<h3>Methods Complexity</h3><ul>")
                for method_name, complexity in data.get('methods_complexity', {}).items():
                    f.write(f"<li>{method_name}: Cyclomatic Complexity = {complexity}</li>")
                f.write("</ul>")
                f.write("<h3>Inter-Service Calls</h3><ul>")
                for service in data.get('inter_service_calls', []):
                    f.write(f"<li>{service}</li>")
                f.write("</ul>")
                f.write("<h3>Relationships</h3><ul>")
                for rel in data['relationships']:
                    f.write(f"<li>{rel['type']} -> {rel['target']} ({rel['relationship_type']})</li>")
                f.write("</ul>")
                # Additional sections for new features
                if data.get('database_entities'):
                    f.write("<h3>Database Entities</h3><ul>")
                    for entity in data['database_entities']:
                        f.write(f"<li>Table: {entity.get('table_name', 'Unknown')}</li>")
                        f.write("<ul>")
                        for field in entity.get('fields', []):
                            f.write(f"<li>Field: {field['name']} ({field['type']})</li>")
                        f.write("</ul>")
                    f.write("</ul>")
                if data.get('messaging_usage'):
                    f.write("<h3>Messaging Usage</h3><ul>")
                    for usage in data['messaging_usage']:
                        f.write(f"<li>{usage['usage']} via {usage['type']} in method {usage.get('method', '')}</li>")
                    f.write("</ul>")
                if data.get('external_api_calls'):
                    f.write("<h3>External API Calls</h3><ul>")
                    for api_call in data['external_api_calls']:
                        f.write(f"<li>{api_call['usage']} to {api_call.get('target', 'Unknown')} in method {api_call.get('method', '')}</li>")
                    f.write("</ul>")
                if data.get('config_properties'):
                    f.write("<h3>Configuration Properties</h3><ul>")
                    for prop in data['config_properties']:
                        f.write(f"<li>Field {prop['field']} uses property {prop['property']}</li>")
                    f.write("</ul>")
                if 'spring_beans' in data:
                    f.write("<h3>Spring Beans</h3><ul>")
                    for bean in data['spring_beans']:
                        f.write(f"<li>{bean['name']} ({bean['return_type']})</li>")
                    f.write("</ul>")
                if 'api_endpoints' in data:
                    f.write("<h3>API Endpoints</h3><ul>")
                    for endpoint in data['api_endpoints']:
                        f.write(f"<li>{endpoint}</li>")
                    f.write("</ul>")
                if 'security' in data and data['security']['secured_methods']:
                    f.write("<h3>Security</h3><ul>")
                    for sec in data['security']['secured_methods']:
                        f.write(f"<li>{sec['method']} - {sec['annotation']}: {sec['value']}</li>")
                    f.write("</ul>")
                if 'aspects' in data:
                    f.write("<h3>Aspects</h3><ul>")
                    for aspect in data['aspects']:
                        for advice in aspect['advices']:
                            f.write(f"<li>{advice['type']} {advice['method']}: {advice['pointcut']}</li>")
                    f.write("</ul>")
                if 'profiles' in data:
                    f.write("<h3>Profiles/Conditions</h3><ul>")
                    for profile in data['profiles']:
                        f.write(f"<li>{profile}</li>")
                    f.write("</ul>")
                if 'database_interactions' in data:
                    f.write("<h3>Database Interactions</h3><ul>")
                    for db in data['database_interactions']:
                        f.write(f"<li>{db['name']}: {db['query']}</li>")
                    f.write("</ul>")
                if 'microservice_calls' in data:
                    f.write("<h3>Microservice Calls</h3><ul>")
                    for call in data['microservice_calls']:
                        f.write(f"<li>{call}</li>")
                    f.write("</ul>")
                if 'documentation_coverage' in data:
                    f.write("<h3>Documentation Coverage</h3>")
                    f.write(f"<p>Methods Documented: {data['documentation_coverage']['documented_methods']} / {data['documentation_coverage']['total_methods']} ({data['documentation_coverage']['coverage']}%)</p>")
                if 'outdated_dependencies' in data:
                    f.write("<h3>Outdated Dependencies</h3><ul>")
                    for dep in data['outdated_dependencies']:
                        f.write(f"<li>{dep['dependency']} - {dep['version']}: {dep['issue']}</li>")
                    f.write("</ul>")
            f.write("</body></html>")
        logger.info("HTML scan summary written to %s", summary_file)
    except Exception:
        logger.exception("Error writing HTML report.")

def detect_bounded_context(java_file_path: Path) -> str:
    """Detect bounded context based on the file path or class annotations."""
    file_path_str = str(java_file_path).lower()

    if "booking" in file_path_str:
        return "Booking Context"
    elif "passenger" in file_path_str:
        return "Passenger Context"
    elif "flight" in file_path_str:
        return "Flight Context"
    else:
        return "Unknown Context"

def extract_api_endpoints(java_class) -> List[str]:
    """Extract API endpoints from a Java class by scanning for annotations like @GetMapping."""
    api_endpoints = []
    class_mappings = []
    # Handle class-level @RequestMapping
    for annotation in java_class.annotations:
        annotation_name = get_annotation_name(annotation)
        if annotation_name == "RequestMapping":
            if annotation.element:
                value = extract_annotation_value(annotation)
                class_mappings.extend(value)

    if java_class.methods:
        for method in java_class.methods:
            method_mappings = []
            http_methods = []
            for annotation in method.annotations:
                annotation_name = get_annotation_name(annotation)
                if annotation_name in {"RequestMapping", "GetMapping", "PostMapping", "PutMapping", "DeleteMapping", "PatchMapping"}:
                    if annotation.element:
                        value = extract_annotation_value(annotation)
                        method_mappings.extend(value)
                    else:
                        method_mappings.append('/')
                    http_methods.append(annotation_name.replace('Mapping', '').upper() if 'Mapping' in annotation_name else annotation_name.upper())
                    # Combine class-level and method-level mappings
                    for class_mapping in class_mappings or ['/']:
                        for method_mapping in method_mappings or ['/']:
                            full_mapping = (class_mapping.rstrip('/') + '/' + method_mapping.lstrip('/')).replace('//', '/')
                            endpoint_info = f"{','.join(http_methods)} {full_mapping}"
                            api_endpoints.append(endpoint_info)

    return api_endpoints

def extract_annotation_value(annotation):
    """Extract the value from an annotation."""
    values = []
    if annotation.element:
        if isinstance(annotation.element, javalang.tree.Literal):
            values.append(annotation.element.value.strip('"\''))
        elif isinstance(annotation.element, javalang.tree.ElementValuePair):
            if isinstance(annotation.element.value, javalang.tree.Literal):
                values.append(annotation.element.value.value.strip('"\''))
        elif isinstance(annotation.element, javalang.tree.ElementArrayValue):
            for element in annotation.element.values:
                if isinstance(element, javalang.tree.Literal):
                    values.append(element.value.strip('"\''))
        elif isinstance(annotation.element, list):
            for elem in annotation.element:
                if isinstance(elem, javalang.tree.ElementValuePair):
                    if isinstance(elem.value, javalang.tree.Literal):
                        values.append(elem.value.value.strip('"\''))
    return values

def extract_domain_from_path(java_file_path: Path) -> str:
    """Extract domain from the file path. This assumes domains are structured in directories."""
    path_parts = str(java_file_path).split(os.sep)

    if "booking" in path_parts:
        return "Booking Domain"
    elif "flight" in path_parts:
        return "Flight Domain"
    elif "passenger" in path_parts:
        return "Passenger Domain"
    else:
        return "Unknown Domain"

# Rest of the code remains the same as previously provided, ensuring all functions are included.

@app.post("/scan_project")
async def scan_project_endpoint(
    project_zip: UploadFile = File(...),
    config_file: Optional[UploadFile] = File(None),
    complexity_threshold: int = Form(10),
    method_length_threshold: int = Form(50),
    class_size_threshold: int = Form(10),
    output_format: str = Form('json')
):
    try:
        if not project_zip:
            return JSONResponse(status_code=400, content={"error": "Please provide a ZIP file of the Java project."})

        # Save the uploaded project ZIP file
        with tempfile.TemporaryDirectory() as temp_dir:
            project_path = temp_dir

            # Handle ZIP file upload
            if project_zip:
                if project_zip.content_type != 'application/zip' and not project_zip.filename.endswith('.zip'):
                    return JSONResponse(status_code=400, content={"error": "Invalid file type. Please upload a ZIP file."})

                project_zip_path = os.path.join(temp_dir, project_zip.filename)
                try:
                    with open(project_zip_path, 'wb') as f:
                        while True:
                            contents = await project_zip.read(1024)
                            if not contents:
                                break
                            f.write(contents)
                except Exception:
                    logger.exception("Error saving uploaded file.")
                    return JSONResponse(status_code=500, content={"error": "Failed to save uploaded file."})

                # Check the size of the saved file
                file_size = os.path.getsize(project_zip_path)
                if file_size > MAX_UPLOAD_SIZE:
                    return JSONResponse(status_code=400, content={"error": f"File size exceeds the maximum allowed size of {MAX_UPLOAD_SIZE / (1024 * 1024)} MB."})

                # Extract the uploaded ZIP file
                try:
                    with zipfile.ZipFile(project_zip_path, 'r') as zip_ref:
                        zip_ref.extractall(temp_dir)
                    logger.info("Project uploaded and extracted successfully!")
                except zipfile.BadZipFile:
                    logger.error("Invalid ZIP file.")
                    return JSONResponse(status_code=400, content={"error": "Invalid ZIP file."})

            # Load configuration if provided
            if config_file:
                config_file_path = os.path.join(temp_dir, config_file.filename)
                try:
                    with open(config_file_path, 'wb') as f:
                        while True:
                            contents = await config_file.read(1024)
                            if not contents:
                                break
                            f.write(contents)
                except Exception:
                    logger.exception("Error saving configuration file.")
                    return JSONResponse(status_code=500, content={"error": "Failed to save configuration file."})

                # Load configuration from the saved file
                with open(config_file_path, 'r') as f:
                    config_content = f.read()
                load_configuration(config_content)
            else:
                logger.warning("Using default configuration parameters.")
                config.update({
                    'complexity_threshold': complexity_threshold,
                    'method_length_threshold': method_length_threshold,
                    'class_size_threshold': class_size_threshold
                })

            # Start scanning the project
            scan_summary = scan_project_async(project_path, output_format)

            # If there was an error, return it
            if "error" in scan_summary:
                return JSONResponse(status_code=500, content={"error": "Error during project scan."})

            # Generate architecture prompt
            architecture_prompt = generate_architecture_prompt(scan_summary)

            # Return the scan results and the architecture prompt
            return JSONResponse(content={
                "scan_summary": scan_summary,
                "architecture_prompt": architecture_prompt
            })
    except Exception:
        logger.exception("Unexpected error in scan_project_endpoint")
        return JSONResponse(status_code=500, content={"error": "Internal server error."})
