import os
import zipfile
import tempfile
import javalang
import yaml
import logging
import json
import networkx as nx
from pathlib import Path
from collections import defaultdict
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import JSONResponse, HTMLResponse

# New imports for parsing properties and YAML files
import re

# Load environment variables
load_dotenv()

# Set up the logger with enhanced formatting and handlers
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
# Create handlers
file_handler = logging.FileHandler('groq_api_log.log')
stream_handler = logging.StreamHandler()
# Create formatters and add to handlers
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)
stream_handler.setFormatter(formatter)
# Add handlers to the logger
if not logger.handlers:
    logger.addHandler(file_handler)
    logger.addHandler(stream_handler)

# Configure security settings
ALLOWED_FILE_TYPES = ['zip']
MAX_UPLOAD_SIZE = 50 * 1024 * 1024  # 50 MB

GROQ_API_KEY = "your_groq_api_key_here"
if not GROQ_API_KEY:
    logger.error("Groq API key not found. Please set the GROQ_API_KEY environment variable.")

# Initialize FastAPI app
app = FastAPI()

# Configuration and constants
MAX_TOKENS = 5000000  # Token limit for prompt generation
DEFAULT_CONFIG = {
    'annotation_filters': [],
    'dependency_filters': {'include': [], 'exclude': []},
    'file_path_filters': {'include': [], 'exclude': []},
    'complexity_threshold': 10,
    'method_length_threshold': 50,
    'class_size_threshold': 10,
    'batch_size': 50,
    'output_file': 'scan_summary.json'
}
config = DEFAULT_CONFIG.copy()

def count_tokens(text):
    """Estimate the number of tokens in the text."""
    return len(text) / 4

def load_configuration(config_content):
    """Load configuration from uploaded YAML content."""
    global config
    try:
        user_config = yaml.safe_load(config_content)
        config = {**DEFAULT_CONFIG, **user_config}
        logger.info("Successfully loaded configuration.")
    except yaml.YAMLError:
        logger.exception("Error parsing configuration file.")
        config = DEFAULT_CONFIG.copy()
    except Exception:
        logger.exception("Error loading configuration.")
        config = DEFAULT_CONFIG.copy()

def scan_project_async(project_dir: str, output_format: str = 'json') -> Dict[str, Any]:
    """Scan the Java project asynchronously with filtering options."""
    scan_summary = {}
    metadata_cache = {}
    try:
        logger.info("Starting to scan project at %s", project_dir)

        java_files = list(Path(project_dir).rglob("*.java"))

        if not java_files:
            logger.error("No Java files found in the directory.")
            return {"error": "No Java files found in the project."}

        logger.info("Found %d Java files to scan.", len(java_files))

        # Apply file path filters
        java_files = apply_file_path_filters(java_files)

        batch_size = config.get('batch_size', 50)
        total_files = len(java_files)
        processed_files = 0

        # Parse configuration files
        configurations = parse_configuration_files(project_dir)

        # Parse dependency files
        dependencies = parse_dependency_files(project_dir)

        # Use ThreadPoolExecutor for concurrent scanning
        with ThreadPoolExecutor(max_workers=os.cpu_count() or 1) as executor:
            futures = []
            for batch in [java_files[i:i + batch_size] for i in range(0, len(java_files), batch_size)]:
                futures.append(executor.submit(scan_batch, batch, scan_summary, metadata_cache, configurations, dependencies))

            # Update progress as futures complete
            for future in as_completed(futures):
                try:
                    future.result()
                    processed_files += batch_size
                    progress = processed_files / total_files
                    progress_percent = min(progress, 1.0)*100
                    logger.info("Progress: %.2f%%", progress_percent)
                except Exception:
                    logger.exception("Batch generated an exception.")

        # Detect cyclic dependencies after scanning all files
        detect_cyclic_dependencies(scan_summary)
        generate_report(output_format, project_dir, scan_summary)
    except Exception:
        logger.exception("Error during project scan.")
        return {"error": "Error during project scan."}

    return scan_summary

def parse_configuration_files(project_dir: str) -> Dict[str, Any]:
    """Parse application.properties and application.yml files."""
    configurations = {}
    # Parse application.properties files
    properties_files = list(Path(project_dir).rglob("application*.properties"))
    for prop_file in properties_files:
        try:
            with open(prop_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                for line in lines:
                    line = line.strip()
                    # Skip comments and empty lines
                    if not line or line.startswith('#') or '=' not in line:
                        continue
                    key, value = line.split('=', 1)
                    configurations[key.strip()] = value.strip()
        except Exception:
            logger.exception(f"Error parsing properties file {prop_file}")

    # Parse application.yml files
    yml_files = list(Path(project_dir).rglob("application*.yml")) + list(Path(project_dir).rglob("application*.yaml"))
    for yml_file in yml_files:
        try:
            with open(yml_file, 'r', encoding='utf-8') as f:
                yml_content = yaml.safe_load(f)
                if yml_content:
                    configurations.update(flatten_dict(yml_content))
        except Exception:
            logger.exception(f"Error parsing YAML file {yml_file}")

    logger.info("Configuration files parsed.")
    return configurations

def flatten_dict(d, parent_key='', sep='.'):
    """Flatten nested dictionaries into a single-level dictionary."""
    items = []
    for k, v in d.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_dict(v, new_key, sep=sep).items())
        else:
            items.append((new_key, v))
    return dict(items)

def parse_dependency_files(project_dir: str) -> Dict[str, Any]:
    """Parse pom.xml or build.gradle files to extract dependencies."""
    dependencies = {}
    # Parse pom.xml files
    pom_files = list(Path(project_dir).rglob("pom.xml"))
    for pom_file in pom_files:
        try:
            with open(pom_file, 'r', encoding='utf-8') as f:
                content = f.read()
                dependency_matches = re.findall(r'<dependency>.*?</dependency>', content, re.DOTALL)
                for dep in dependency_matches:
                    group_id = re.search(r'<groupId>(.*?)</groupId>', dep)
                    artifact_id = re.search(r'<artifactId>(.*?)</artifactId>', dep)
                    version = re.search(r'<version>(.*?)</version>', dep)
                    if group_id and artifact_id and version:
                        key = f"{group_id.group(1)}:{artifact_id.group(1)}"
                        dependencies[key] = version.group(1)
        except Exception:
            logger.exception(f"Error parsing pom.xml file {pom_file}")

    logger.info("Dependency files parsed.")
    return dependencies

def apply_file_path_filters(java_files: List[Path]) -> List[Path]:
    """Apply include/exclude filters to the list of Java files based on file paths."""
    include_filters = config['file_path_filters'].get('include', [])
    exclude_filters = config['file_path_filters'].get('exclude', [])

    filtered_files = []
    for java_file in java_files:
        file_str = str(java_file)
        if include_filters and not any(pattern in file_str for pattern in include_filters):
            continue
        if any(pattern in file_str for pattern in exclude_filters):
            continue
        filtered_files.append(java_file)

    logger.info("After applying filters, %d Java files will be scanned.", len(filtered_files))
    return filtered_files

def scan_batch(file_batch: List[Path], scan_summary, metadata_cache, configurations, dependencies):
    """Scan a batch of Java files."""
    for java_file in file_batch:
        scan_file(java_file, scan_summary, metadata_cache, configurations, dependencies)

def scan_file(java_file_path: Path, scan_summary, metadata_cache, configurations, dependencies):
    """Scan a single Java file for services, annotations, and dependencies."""
    try:
        with open(java_file_path, 'r', encoding='utf-8') as file:
            java_code = file.read()

        tree = javalang.parse.parse(java_code)

        for node in tree.types:
            if isinstance(node, javalang.tree.ClassDeclaration):
                scan_class(node, java_file_path, scan_summary, metadata_cache, configurations, dependencies)

        imports = extract_imports(tree)
        logger.debug("Imports found in %s: %s", java_file_path, imports)

    except javalang.parser.JavaSyntaxError:
        logger.exception(f"Syntax error in {java_file_path}")
    except Exception:
        logger.exception(f"Error parsing {java_file_path}")

def scan_class(java_class, java_file_path: Path, scan_summary, metadata_cache, configurations, dependencies):
    """Scan a Java class for advanced analysis: annotations, complexity, dependencies, and relationships."""
    try:
        class_name = java_class.name
        last_modified = os.path.getmtime(java_file_path)

        if class_name in metadata_cache and metadata_cache[class_name]['last_modified'] >= last_modified:
            logger.info("Skipping cached class %s", class_name)
            return

        class_annotations = extract_annotations(java_class)
        if not apply_annotation_filters(class_annotations):
            logger.info("Class %s filtered out by annotation filter", class_name)
            return

        # Determine if the class is a test class
        is_test_class = 'Test' in class_annotations or java_file_path.parts[-2].lower() == 'test'

        # Extract bounded context, API endpoints, and domain information
        bounded_context = detect_bounded_context(java_file_path)
        api_endpoints = extract_api_endpoints(java_class)
        domain = extract_domain_from_path(java_file_path)

        # Advanced analysis: Calculate cyclomatic complexity and detect code smells
        methods_complexity = calculate_methods_complexity(java_class)
        code_smells = detect_code_smells(java_class, methods_complexity, is_test_class)

        # Collect metadata for the class
        metadata = {
            'annotations': class_annotations,
            'dependencies': [],
            'inter_service_calls': [],
            'fields': extract_fields(java_class),
            'methods': extract_methods(java_class),
            'relationships': [],
            'file_path': str(java_file_path),
            'last_modified': last_modified,
            'component_type': detect_spring_component_type(java_class),
            'bounded_context': bounded_context,
            'api_endpoints': api_endpoints,
            'domain': domain,
            'methods_complexity': methods_complexity,
            'code_smells': code_smells,
            'is_test_class': is_test_class,
            'database_entities': [],  # New field for entities
            'messaging_usage': [],    # New field for messaging
            'external_api_calls': [], # New field for external API calls
            'config_properties': [],  # New field for config properties
            'layer': detect_layer(java_file_path),  # New field for layer identification
            'transactions': detect_transactions(java_class),  # New field for transactions
            'global_state_usage': detect_global_state(java_class),  # New field for global state
        }

        # Detect service injections and relationships
        detect_service_injections(java_class, metadata)
        metadata['relationships'] = detect_relationships(java_class, metadata)

        # Extract Spring Beans
        if metadata['component_type'] in ["Configuration", "Component"]:
            beans = extract_spring_beans(java_class)
            metadata['spring_beans'] = beans

        # Security Analysis
        security_info = analyze_security(java_class)
        metadata['security'] = security_info

        # AOP Analysis
        aspects = detect_aspects(java_class)
        if aspects:
            metadata['aspects'] = aspects

        # Profile and Conditional Beans
        profiles = extract_profiles(java_class)
        if profiles:
            metadata['profiles'] = profiles

        # Microservice Communication Analysis
        inter_service_calls = detect_inter_service_calls(java_class)
        if inter_service_calls:
            metadata['microservice_calls'] = inter_service_calls

        # Database Interaction Analysis
        db_interactions = analyze_database_interactions(java_class)
        if db_interactions:
            metadata['database_interactions'] = db_interactions

        # Database Entity Extraction (New)
        database_entities = extract_database_entities(java_class)
        if database_entities:
            metadata['database_entities'] = database_entities

        # Messaging Patterns Detection (New)
        messaging_usage = detect_messaging_usage(java_class)
        if messaging_usage:
            metadata['messaging_usage'] = messaging_usage

        # External API Calls Detection (New)
        external_api_calls = detect_external_api_calls(java_class)
        if external_api_calls:
            metadata['external_api_calls'] = external_api_calls

        # Configuration Properties Extraction (New)
        config_properties = extract_config_properties(java_class)
        if config_properties:
            metadata['config_properties'] = config_properties

        # Module Coupling Analysis (New)
        module_coupling = analyze_module_coupling(java_class)
        if module_coupling:
            metadata['module_coupling'] = module_coupling

        # Documentation Analysis
        documentation_coverage = analyze_documentation(java_class)
        metadata['documentation_coverage'] = documentation_coverage

        # Dependency Version Checks
        outdated_dependencies = check_dependency_versions(dependencies)
        metadata['outdated_dependencies'] = outdated_dependencies

        # Cache and summarize the metadata
        metadata_cache[class_name] = metadata
        scan_summary[class_name] = metadata

        logger.debug("Class %s scanned with metadata.", class_name)
    except Exception:
        logger.exception(f"Error scanning class {java_class.name}")

def extract_imports(tree):
    """Extract imports from the CompilationUnit."""
    imports = []
    if hasattr(tree, 'imports'):
        for imp in tree.imports:
            imports.append(imp.path)
    return imports

def detect_spring_component_type(java_class):
    """Detect the Spring component type of the Java class based on its annotations."""
    component_types = [
        "SpringBootApplication", "Configuration", "RestController", "Controller",
        "Service", "Repository", "Component"
    ]

    for annotation in java_class.annotations:
        annotation_name = get_annotation_name(annotation)
        if annotation_name in component_types:
            return annotation_name

    return "Unknown"

def detect_service_injections(java_class, metadata):
    """Detect field-level, constructor-based, and method-based service injections."""
    # Field injection
    if java_class.fields:
        for field in java_class.fields:
            if field.annotations:
                for annotation in field.annotations:
                    annotation_name = get_annotation_name(annotation)
                    if annotation_name in ["Autowired", "Inject", "Resource"]:
                        injected_service = field.type.name if field.type and field.type.name else 'Unknown'
                        metadata['inter_service_calls'].append(injected_service)
                        logger.debug("Injected service found in %s: %s", java_class.name, injected_service)

    # Constructor injection
    if java_class.constructors:
        for constructor in java_class.constructors:
            if constructor.annotations:
                for annotation in constructor.annotations:
                    annotation_name = get_annotation_name(annotation)
                    if annotation_name in ["Autowired", "Inject"]:
                        for param in constructor.parameters:
                            injected_service = param.type.name if param.type and param.type.name else 'Unknown'
                            metadata['inter_service_calls'].append(injected_service)
                            logger.debug("Injected service found in constructor of %s: %s", java_class.name, injected_service)

    # Method injection
    if java_class.methods:
        for method in java_class.methods:
            if method.annotations:
                for annotation in method.annotations:
                    annotation_name = get_annotation_name(annotation)
                    if annotation_name in ["Autowired", "Inject"]:
                        for param in method.parameters:
                            injected_service = param.type.name if param.type and param.type.name else 'Unknown'
                            metadata['inter_service_calls'].append(injected_service)
                            logger.debug("Injected service found in method %s of %s: %s", method.name, java_class.name, injected_service)

def extract_annotations(java_class):
    """Extract annotations from a Java class."""
    return [get_annotation_name(annotation) for annotation in java_class.annotations]

def get_annotation_name(annotation):
    """Get the name of an annotation."""
    if isinstance(annotation.name, str):
        return annotation.name
    elif hasattr(annotation.name, 'qualifier') and annotation.name.qualifier:
        return f"{annotation.name.qualifier}.{annotation.name.member}"
    else:
        return annotation.name.member

def extract_fields(java_class):
    """Extract fields and their metadata from a Java class."""
    fields = []
    if java_class.fields:
        for field in java_class.fields:
            field_info = {
                'names': [declarator.name for declarator in field.declarators],
                'type': field.type.name if field.type and isinstance(field.type.name, str) else 'Unknown',
                'annotations': [get_annotation_name(annotation) for annotation in field.annotations],
                'modifiers': list(field.modifiers)
            }
            fields.append(field_info)
    return fields

def extract_methods(java_class):
    """Extract methods from a Java class."""
    methods = []
    if java_class.methods:
        for method in java_class.methods:
            try:
                method_info = {
                    'name': method.name,
                    'return_type': method.return_type.name if method.return_type and isinstance(method.return_type.name, str) else 'void',
                    'parameters': [{'name': param.name,
                                    'type': param.type.name if param.type and isinstance(param.type.name, str) else 'Unknown'}
                                   for param in method.parameters],
                    'annotations': [get_annotation_name(annotation) for annotation in method.annotations],
                    'modifiers': list(method.modifiers),
                    'body_length': len(method.body) if method.body else 0
                }
                methods.append(method_info)
            except Exception:
                logger.exception("Error extracting method information.")
    return methods

# The rest of the code remains the same, including all the functions
# such as detect_cyclic_dependencies, generate_report, detect_bounded_context,
# extract_api_endpoints, extract_annotation_value, extract_domain_from_path,
# generate_html_report, generate_architecture_prompt, extract_database_entities,
# detect_entity_relationships, detect_messaging_usage, detect_external_api_calls,
# extract_config_properties, detect_layer, detect_transactions, detect_global_state,
# analyze_module_coupling, apply_annotation_filters, detect_relationships,
# detect_method_calls, detect_data_flow, calculate_methods_complexity,
# calculate_cyclomatic_complexity, detect_code_smells, analyze_documentation,
# check_dependency_versions.

# For brevity, these functions are not repeated here, but ensure that in your code,
# you include all of them with the same exception handling pattern, using logger.exception()
# where appropriate.

# Finally, the FastAPI endpoint remains the same, with exception handling added.

@app.post("/scan_project")
async def scan_project_endpoint(
    project_zip: UploadFile = File(...),
    config_file: Optional[UploadFile] = File(None),
    complexity_threshold: int = Form(10),
    method_length_threshold: int = Form(50),
    class_size_threshold: int = Form(10),
    output_format: str = Form('json')
):
    try:
        if not project_zip:
            return JSONResponse(status_code=400, content={"error": "Please provide a ZIP file of the Java project."})

        # Save the uploaded project ZIP file
        with tempfile.TemporaryDirectory() as temp_dir:
            project_path = temp_dir

            # Handle ZIP file upload
            if project_zip:
                if project_zip.content_type != 'application/zip' and not project_zip.filename.endswith('.zip'):
                    return JSONResponse(status_code=400, content={"error": "Invalid file type. Please upload a ZIP file."})

                project_zip_path = os.path.join(temp_dir, project_zip.filename)
                try:
                    with open(project_zip_path, 'wb') as f:
                        while True:
                            contents = await project_zip.read(1024)
                            if not contents:
                                break
                            f.write(contents)
                except Exception:
                    logger.exception("Error saving uploaded file.")
                    return JSONResponse(status_code=500, content={"error": "Failed to save uploaded file."})

                # Check the size of the saved file
                file_size = os.path.getsize(project_zip_path)
                if file_size > MAX_UPLOAD_SIZE:
                    return JSONResponse(status_code=400, content={"error": f"File size exceeds the maximum allowed size of {MAX_UPLOAD_SIZE / (1024 * 1024)} MB."})

                # Extract the uploaded ZIP file
                try:
                    with zipfile.ZipFile(project_zip_path, 'r') as zip_ref:
                        zip_ref.extractall(temp_dir)
                    logger.info("Project uploaded and extracted successfully!")
                except zipfile.BadZipFile:
                    logger.error("Invalid ZIP file.")
                    return JSONResponse(status_code=400, content={"error": "Invalid ZIP file."})

            # Load configuration if provided
            if config_file:
                config_file_path = os.path.join(temp_dir, config_file.filename)
                try:
                    with open(config_file_path, 'wb') as f:
                        while True:
                            contents = await config_file.read(1024)
                            if not contents:
                                break
                            f.write(contents)
                except Exception:
                    logger.exception("Error saving configuration file.")
                    return JSONResponse(status_code=500, content={"error": "Failed to save configuration file."})

                # Load configuration from the saved file
                with open(config_file_path, 'r') as f:
                    config_content = f.read()
                load_configuration(config_content)
            else:
                logger.warning("Using default configuration parameters.")
                config.update({
                    'complexity_threshold': complexity_threshold,
                    'method_length_threshold': method_length_threshold,
                    'class_size_threshold': class_size_threshold
                })

            # Start scanning the project
            scan_summary = scan_project_async(project_path, output_format)

            # If there was an error, return it
            if "error" in scan_summary:
                return JSONResponse(status_code=500, content={"error": "Error during project scan."})

            # Generate architecture prompt
            architecture_prompt = generate_architecture_prompt(scan_summary)

            # Return the scan results and the architecture prompt
            return JSONResponse(content={
                "scan_summary": scan_summary,
                "architecture_prompt": architecture_prompt
            })
    except Exception:
        logger.exception("Unexpected error in scan_project_endpoint")
        return JSONResponse(status_code=500, content={"error": "Internal server error."})
