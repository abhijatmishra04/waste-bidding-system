import json
import logging
from typing import Dict, Any, List, Generator

import tiktoken

# Configure logger
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# Constants
TOKEN_LIMIT = 8000             # Max tokens per chunk
CHUNK_SIZE = 3                 # How many relationships/endpoints to show per batch
USE_SINGLE_PASS = True         # If True, try single-pass approach before chunking
MODEL_NAME = "gpt-4"           # or "cl100k_base"
OUTPUT_FILE = "C://architecture_prompts.txt"
TRACKER_FILE = "C://progress_tracker.txt"

# Attempt to create a tokenizer for your chosen model
try:
    tokenizer = tiktoken.encoding_for_model(MODEL_NAME)
except KeyError:
    logger.warning(f"Model '{MODEL_NAME}' not recognized by tiktoken, falling back to cl100k_base.")
    tokenizer = tiktoken.get_encoding("cl100k_base")

def count_tokens(text: str) -> int:
    """Returns the number of tokens in the given text using the chosen tokenizer."""
    return len(tokenizer.encode(text))

def load_json(file_path: str) -> Dict[str, Any]:
    """Loads JSON data from a file."""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            data = json.load(file)
        logger.info("JSON file loaded successfully.")
        return data
    except Exception as e:
        logger.error(f"Error loading JSON file '{file_path}': {e}")
        raise

def get_last_processed_index() -> int:
    """Reads the last processed index from the tracker file. If not found or error, returns 0."""
    try:
        with open(TRACKER_FILE, 'r') as file:
            last_index = int(file.read().strip())
            logger.info(f"Resuming from index: {last_index}")
            return last_index
    except FileNotFoundError:
        logger.info("Tracker file not found. Starting from the beginning (index 0).")
        return 0
    except Exception as e:
        logger.error(f"Error reading tracker file: {e}")
        return 0

def update_last_processed_index(index: int) -> None:
    """Updates the tracker file with the last processed index."""
    try:
        with open(TRACKER_FILE, 'w') as file:
            file.write(str(index))
        logger.info(f"Tracker updated. Last processed index: {index}")
    except Exception as e:
        logger.error(f"Error updating tracker file: {e}")

def chunk_list(items: List[Any], chunk_size: int = CHUNK_SIZE) -> Generator[List[Any], None, None]:
    """
    Yields sublists of length 'chunk_size'.
    For example: [1,2,3,4,5] -> yields [1,2,3], then [4,5] if chunk_size=3.
    """
    for i in range(0, len(items), chunk_size):
        yield items[i:i + chunk_size]

def build_class_text(class_name: str, metadata: Dict[str, Any]) -> str:
    """
    Build a single large text block for this class.
    If USE_SINGLE_PASS is True and this entire block fits under TOKEN_LIMIT,
    we'll output it in one shot.
    """
    lines = []

    # 1) Basic class info
    class_info = f"Class: {class_name}\n"
    file_path = f"File Path: {metadata.get('file_path', 'Unknown')}\n"
    component_type = f"Component Type: {metadata.get('component_type', 'Unknown')}\n"
    layer = f"Layer: {metadata.get('layer', 'Unknown')}\n"
    annotations = metadata.get("annotations", [])
    annotation_info = "Annotations: None\n"
    if annotations:
        annotation_info = f"Annotations: {', '.join(annotations[:3])}\n"  # up to 3

    lines.extend([class_info, file_path, component_type, layer, annotation_info])

    # 2) Methods complexity
    methods_complexity = metadata.get("methods_complexity", {})
    if methods_complexity:
        lines.append("Methods Complexity:\n")
        for method, complexity in methods_complexity.items():
            lines.append(f"  - {method}: Cyclomatic Complexity = {complexity}\n")

    # 3) Relationships
    relationships = metadata.get("relationships", [])
    if relationships:
        lines.append("Relationships:\n")
        # We still break them into lines of CHUNK_SIZE, but inside a single text block
        for rel_chunk in chunk_list(relationships, CHUNK_SIZE):
            for rel in rel_chunk:
                lines.append(f"  - {rel.get('type', 'Unknown')} {rel.get('target', 'Unknown')} "
                             f"({rel.get('relationship_type', 'Unknown')})\n")

    # 4) API Endpoints
    api_endpoints = metadata.get("api_endpoints", [])
    if api_endpoints:
        lines.append("API Endpoints:\n")
        for ep_chunk in chunk_list(api_endpoints, CHUNK_SIZE):
            lines.append(f"  - {', '.join(ep_chunk)}\n")

    return "".join(lines)

def generate_chunked_prompts(
    scan_summary: Dict[str, Any],
    token_limit: int = TOKEN_LIMIT,
    start_index: int = 0
) -> List[str]:
    """
    Generates architecture prompts in chunks based on the token limit.

    If USE_SINGLE_PASS is True, we try to build a single text block for each class:
      - If the entire block fits under token_limit, we add it in one shot.
      - If not, we fallback to chunking line-by-line.

    Otherwise, we always do line-by-line chunking.
    """
    prompts: List[str] = []
    current_prompt: List[str] = []
    current_tokens = 0
    index = start_index

    items = list(scan_summary.items())[start_index:]
    logger.info(f"Starting generation from index {start_index} with {len(items)} items remaining.")

    for class_name, metadata in items:
        if USE_SINGLE_PASS:
            # Build one big block
            class_block = build_class_text(class_name, metadata)
            block_tokens = count_tokens(class_block)

            if block_tokens < token_limit:
                # We can safely add the entire block at once
                # If adding it exceeds current chunk, flush first
                if current_tokens + block_tokens >= token_limit:
                    logger.info(f"Reached token limit at index {index}. Writing chunk.")
                    prompts.append("".join(current_prompt))
                    update_last_processed_index(index)
                    current_prompt = []
                    current_tokens = 0

                current_prompt.append(class_block)
                current_tokens += block_tokens

            else:
                # Fallback to chunking line-by-line
                logger.info(f"Class '{class_name}' exceeds single-pass limit. Chunking it manually.")
                chunk_class_data_line_by_line(
                    class_name, metadata, token_limit,
                    prompts, current_prompt, index
                )
                # Recalculate current_tokens after chunking
                current_tokens = sum(count_tokens(part) for part in current_prompt)

        else:
            # Always line-by-line
            chunk_class_data_line_by_line(
                class_name, metadata, token_limit,
                prompts, current_prompt, index
            )
            current_tokens = sum(count_tokens(part) for part in current_prompt)

        # If we are at or above the limit after adding this class, flush
        if current_tokens >= token_limit:
            logger.info(f"Reached token limit at index {index}. Writing chunk.")
            prompts.append("".join(current_prompt))
            update_last_processed_index(index)
            current_prompt = []
            current_tokens = 0

        # Increment the index
        index += 1

    # Final flush
    if current_prompt:
        logger.info("Writing final chunk.")
        prompts.append("".join(current_prompt))
        update_last_processed_index(index)

    return prompts

def chunk_class_data_line_by_line(
    class_name: str,
    metadata: Dict[str, Any],
    token_limit: int,
    prompts: List[str],
    current_prompt: List[str],
    index: int
):
    """
    Fallback method: chunk the class's data line-by-line, respecting token_limit.
    We'll parse out the same data fields, but add them piecewise.
    """
    def flush():
        logger.info(f"Reached token limit at index {index}. Writing chunk.")
        prompts.append("".join(current_prompt))
        update_last_processed_index(index)
        current_prompt.clear()

    # We'll track the current token usage by re-counting whenever we append
    def current_token_count() -> int:
        return sum(count_tokens(part) for part in current_prompt)

    # 1) Basic info
    lines: List[str] = []
    lines.append(f"Class: {class_name}\n")
    lines.append(f"File Path: {metadata.get('file_path', 'Unknown')}\n")
    lines.append(f"Component Type: {metadata.get('component_type', 'Unknown')}\n")
    lines.append(f"Layer: {metadata.get('layer', 'Unknown')}\n")
    annotations = metadata.get("annotations", [])
    annotation_info = "Annotations: None\n"
    if annotations:
        annotation_info = f"Annotations: {', '.join(annotations[:3])}\n"
    lines.append(annotation_info)

    # Add each line for basic info
    for detail in lines:
        tokens_needed = count_tokens(detail)
        if current_token_count() + tokens_needed >= token_limit:
            flush()
        current_prompt.append(detail)

    # 2) Methods complexity
    methods_complexity = metadata.get("methods_complexity", {})
    if methods_complexity:
        heading = "Methods Complexity:\n"
        if current_token_count() + count_tokens(heading) >= token_limit:
            flush()
        current_prompt.append(heading)

        for method, complexity in methods_complexity.items():
            line = f"  - {method}: Cyclomatic Complexity = {complexity}\n"
            tokens_needed = count_tokens(line)
            if current_token_count() + tokens_needed >= token_limit:
                flush()
            current_prompt.append(line)

    # 3) Relationships
    relationships = metadata.get("relationships", [])
    if relationships:
        heading = "Relationships:\n"
        if current_token_count() + count_tokens(heading) >= token_limit:
            flush()
        current_prompt.append(heading)

        # batch them in CHUNK_SIZE
        for rel_chunk in chunk_list(relationships, CHUNK_SIZE):
            for rel in rel_chunk:
                line = f"  - {rel.get('type', 'Unknown')} {rel.get('target', 'Unknown')} " \
                       f"({rel.get('relationship_type', 'Unknown')})\n"
                tokens_needed = count_tokens(line)
                if current_token_count() + tokens_needed >= token_limit:
                    flush()
                current_prompt.append(line)

    # 4) API endpoints
    api_endpoints = metadata.get("api_endpoints", [])
    if api_endpoints:
        heading = "API Endpoints:\n"
        if current_token_count() + count_tokens(heading) >= token_limit:
            flush()
        current_prompt.append(heading)

        for ep_chunk in chunk_list(api_endpoints, CHUNK_SIZE):
            line = f"  - {', '.join(ep_chunk)}\n"
            tokens_needed = count_tokens(line)
            if current_token_count() + tokens_needed >= token_limit:
                flush()
            current_prompt.append(line)

def save_prompts_to_file(prompts: List[str], output_file: str) -> None:
    """Saves generated prompts to a file in append mode."""
    try:
        with open(output_file, 'a', encoding='utf-8') as file:
            for prompt in prompts:
                file.write(prompt + "\n" + ("=" * 80) + "\n")
        logger.info(f"Prompts written to {output_file}.")
    except Exception as e:
        logger.error(f"Error writing prompts to file: {e}")

def main():
    """
    Main function to process the JSON, generate prompts with optional single-pass approach,
    and save to file with chunking logic if needed.
    """
    json_file_path = r"C:\combined_analysis.json"  # Adjust path as needed
    try:
        scan_summary = load_json(json_file_path)
        start_index = get_last_processed_index()

        # Generate prompts from the last processed index
        prompts = generate_chunked_prompts(scan_summary, token_limit=TOKEN_LIMIT, start_index=start_index)

        save_prompts_to_file(prompts, OUTPUT_FILE)

    except Exception as e:
        logger.error(f"Error processing JSON: {e}")

if __name__ == "__main__":
    main()
